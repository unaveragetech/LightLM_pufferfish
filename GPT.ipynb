{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc9757b-d90a-4ca3-ac3e-31d308f68b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import tiktoken\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdfbb9a-8ace-477b-af6b-21be8903dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50304 #50257\n",
    "batch_size = 2**19\n",
    "mini_batches = 8\n",
    "time_stamps = 512\n",
    "context_len = 1024\n",
    "emb_neur = 768\n",
    "epochs = 50\n",
    "num_blocks = 12\n",
    "num_heads = 12\n",
    "dropout_neur = 0.2\n",
    "data_dir = \n",
    "\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_lr_steps = 700\n",
    "weight_decay = 0.1\n",
    "beta1, beta2 = 0.9, 0.95\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 \n",
    "if ddp:\n",
    "    init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "else:\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"device: {device}\")\n",
    "    \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba70c2c-1dba-4868-9fbb-d7fc8e83a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(epoch):\n",
    "    if epoch < lr_steps:\n",
    "        return (max_lr * (epoch+1)/lr_steps)\n",
    "    if epoch > epochs:\n",
    "        return min_lr\n",
    "    loc = (epoch - lr_steps)/(epochs - lr_steps)\n",
    "    coef = 0.5 * (1.0 + math.cos(math.pi * loc))\n",
    "    return min_lr + coef * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c7aa4be-a0d2-49bc-a705-24b06aeae8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert batch_size % (mini_batches * time_stamps * ddp_world_size) == 0, \"batch_size is not devided by B and T and number_of_gpus\"\n",
    "mini_epochs = int(batch_size / (mini_batches * time_stamps * ddp_world_size)) #number of mini-batches to get 0.5M batch\n",
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, B, T, cur_process, num_processes, data_dir, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.cur_process = cur_process\n",
    "        self.cur_shard = 0\n",
    "        self.num_processes = num_processes\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        shards = os.listdir(self.data_dir)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(self.data_dir, s) for s in shards]\n",
    "        self.shards = shards\n",
    "\n",
    "        self.tokens = load_tokens(self.shards[self.cur_shard])\n",
    "        \n",
    "        self.current_step = cur_process * B * T\n",
    "\n",
    "        print(f\"loaded {len(text)} tokens\")\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.cur_process\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        \n",
    "        self.current_step += B * T * self.num_processes\n",
    "        tokens = self.tokens[self.current_step:self.current_step+B*T+1]\n",
    "        x = (tokens[:-1]).view(B, T)\n",
    "        y = (tokens[1:]).view(B, T)\n",
    "        if (self.current_step+B*T* self.num_processes + B*T+1)  > len(self.tokens):\n",
    "            self.cur_shard = (self.cur_shard+1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.cur_shard])\n",
    "            self.current_step = self.cur_process * B * T\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23426d49-2a88-4082-90d8-c93a07006b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(emb_neur, 3 * emb_neur)\n",
    "        self.proj = nn.Linear(emb_neur, emb_neur)\n",
    "        self.proj.COMES_TO_RESIDUAL = 1\n",
    "        # self.dropout = nn.Dropout(dropout_neur)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        assert emb_neur % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        B, T, C = idx.shape\n",
    "        qkv = self.qkv(idx)\n",
    "        q, k, v = qkv.split(emb_neur, dim=2)\n",
    "        q = q.view(B, T, num_heads, C//num_heads).transpose(1, 2) # B, nh, T, hs\n",
    "        k = k.view(B, T, num_heads, C//num_heads).transpose(1, 2) # B, nh, T, hs\n",
    "        v = v.view(B, T, num_heads, C//num_heads).transpose(1, 2) # B, nh, T, hs\n",
    "\n",
    "        # attention = q @ k.transpose(-2, -1) * (1.0 / math.sqrt(k.shape[-1]))\n",
    "        # attention = torch.tril(attention[:, :, :T, :T])\n",
    "        \n",
    "        # attention = attention.masked_fill(attention == 0, float(\"-inf\"))\n",
    "        # attention = F.softmax(attention, dim=-1)\n",
    "        # out = attention @ v # B, nh, T, hs \n",
    "        \n",
    "\n",
    "        attention = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        out = attention.transpose(2, 1).contiguous().view(B, T, C)\n",
    "        out = self.proj(out)\n",
    "        # out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.Linear(emb_neur, 4 * emb_neur),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(4 * emb_neur, emb_neur),\n",
    "        #     nn.Dropout(dropout_neur),\n",
    "        # )\n",
    "        self.upl = nn.Linear(emb_neur, 4 * emb_neur)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dwnl = nn.Linear(4 * emb_neur, emb_neur)\n",
    "        self.dwnl.COMES_TO_RESIDUAL = 1\n",
    "\n",
    "    def forward(self, idx):\n",
    "        idx = self.upl(idx)\n",
    "        idx = self.gelu(idx)\n",
    "        idx = self.dwnl(idx)\n",
    "        return idx\n",
    "        # return self.net(idx)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.attentions = SelfAttention(num_heads)\n",
    "        self.ffn = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(emb_neur)\n",
    "        self.ln2 = nn.LayerNorm(emb_neur)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        idx = idx + self.attentions(self.ln1(idx))\n",
    "        idx = idx + self.ffn(self.ln2(idx))\n",
    "        return idx\n",
    "\n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokens_embedding = nn.Embedding(vocab_size, emb_neur)\n",
    "        self.position_embedding = nn.Embedding(context_len, emb_neur)\n",
    "        self.blocks = nn.Sequential( *[Block(num_heads) for _ in range(num_blocks)])\n",
    "        self.ln = nn.LayerNorm(emb_neur)\n",
    "        self.ll_head = nn.Linear(emb_neur, vocab_size)\n",
    "\n",
    "        self.tokens_embedding.weight = self.ll_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        std = (1.0 / math.sqrt(emb_neur))\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, \"COMES_TO_RESIDUAL\"):\n",
    "                std *= (1.0)/(math.sqrt(2*num_blocks))\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    # I have taken this function [configure_optimizers] from Karpathy's nanoGPT\n",
    "    # https://github.com/karpathy/nanoGPT\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        embedded_tokens = self.tokens_embedding(idx) # B, T, emb_neur\n",
    "        embedded_position = self.position_embedding(torch.arange(T, device=device)) # T, emb_neur\n",
    "        \n",
    "        idx = embedded_tokens + embedded_position # B, T, emb_neur\n",
    "        idx = self.blocks(idx)\n",
    "        idx = self.ln(idx)\n",
    "        logits = self.ll_head(idx)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            logits, _ = self.forward(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12ce39f-92fb-4cb9-b8f3-ff8b24600422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124.526208 M parameters\n",
      "loaded 338025 tokens\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 99, with 171,648 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "m = GPT()\n",
    "m = m.to(device)\n",
    "m = torch.compile(m)\n",
    "#making loss average from all gpus\n",
    "if ddp:\n",
    "    m = DDP(model, device_ids=[ddp_local_rank]) \n",
    "raw_m = m.module if ddp else m\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "data_loader = DataLoader(mini_batches, time_stamps, cur_process=ddp_rank, num_processes=ddp_world_size, data_dir=data_dir, split=\"train\")\n",
    "val_loader = DataLoader(mini_batches, time_stamps, cur_process=ddp_rank, num_processes=ddp_world_size, data_dir=data_dir, split=\"val\")\n",
    "# I have taken this function [configure_optimizers] from Karpathy's nanoGPT\n",
    "optmizer = raw_m.configure_optimizers(weight_decay, max_lr, (beta1, beta2), device)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optmizer, T_max=lr_steps, eta_min=min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f0d7e9-84da-4821-9d0b-14f83fe32497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 11.41157, norm: 27.69188, time: 32325.76ms, tok/s: 16218.89\n",
      "epoch: 1, loss: 10.25611, norm: 59.41579, time: 4506.23ms, tok/s: 116347.40\n",
      "epoch: 2, loss: 10.87149, norm: 11.13861, time: 4520.24ms, tok/s: 115986.74\n",
      "epoch: 3, loss: 9.66046, norm: 5.64720, time: 4580.88ms, tok/s: 114451.49\n",
      "epoch: 4, loss: 8.69949, norm: 2.97520, time: 4509.18ms, tok/s: 116271.15\n",
      "epoch: 5, loss: 10.57638, norm: 27.93199, time: 4519.66ms, tok/s: 116001.55\n",
      "epoch: 6, loss: 8.80351, norm: 16.43631, time: 4526.21ms, tok/s: 115833.73\n",
      "epoch: 7, loss: 8.25237, norm: 6.34794, time: 4519.39ms, tok/s: 116008.46\n",
      "epoch: 8, loss: 8.00916, norm: 3.81174, time: 4527.34ms, tok/s: 115804.77\n",
      "epoch: 9, loss: 7.64782, norm: 3.09381, time: 4533.17ms, tok/s: 115655.93\n",
      "epoch: 10, loss: 7.26952, norm: 2.39419, time: 4550.22ms, tok/s: 115222.66\n",
      "epoch: 11, loss: 7.01208, norm: 2.87011, time: 4552.84ms, tok/s: 115156.16\n",
      "epoch: 12, loss: 6.83535, norm: 3.10105, time: 4558.39ms, tok/s: 115015.96\n",
      "epoch: 13, loss: 6.65093, norm: 2.25774, time: 4574.87ms, tok/s: 114601.60\n",
      "epoch: 14, loss: 6.53405, norm: 1.67505, time: 4584.06ms, tok/s: 114371.98\n",
      "epoch: 15, loss: 6.41085, norm: 1.15996, time: 4585.82ms, tok/s: 114328.15\n",
      "epoch: 16, loss: 6.35794, norm: 1.52028, time: 4595.24ms, tok/s: 114093.71\n",
      "epoch: 17, loss: 6.28189, norm: 0.65885, time: 4603.83ms, tok/s: 113880.94\n",
      "epoch: 18, loss: 6.26428, norm: 1.74657, time: 4609.61ms, tok/s: 113738.01\n",
      "epoch: 19, loss: 6.24142, norm: 1.35916, time: 4604.73ms, tok/s: 113858.48\n",
      "epoch: 20, loss: 6.21878, norm: 0.88186, time: 4620.39ms, tok/s: 113472.57\n",
      "epoch: 21, loss: 6.21552, norm: 0.83900, time: 4619.55ms, tok/s: 113493.21\n",
      "epoch: 22, loss: 6.16861, norm: 0.63955, time: 4616.50ms, tok/s: 113568.18\n",
      "epoch: 23, loss: 6.15605, norm: 0.45920, time: 4637.19ms, tok/s: 113061.62\n",
      "epoch: 24, loss: 6.13914, norm: 0.81742, time: 4628.52ms, tok/s: 113273.46\n",
      "epoch: 25, loss: 6.11707, norm: 0.68586, time: 4626.09ms, tok/s: 113332.77\n",
      "epoch: 26, loss: 6.10628, norm: 0.35437, time: 4631.40ms, tok/s: 113202.82\n",
      "epoch: 27, loss: 6.08223, norm: 0.60547, time: 4629.98ms, tok/s: 113237.62\n",
      "epoch: 28, loss: 6.08744, norm: 0.57989, time: 4641.27ms, tok/s: 112962.23\n",
      "epoch: 29, loss: 6.04861, norm: 0.24861, time: 4640.28ms, tok/s: 112986.27\n",
      "epoch: 30, loss: 6.03374, norm: 0.49940, time: 4645.06ms, tok/s: 112869.99\n",
      "epoch: 31, loss: 6.02271, norm: 0.56315, time: 4647.56ms, tok/s: 112809.42\n",
      "epoch: 32, loss: 5.99799, norm: 0.31875, time: 4655.97ms, tok/s: 112605.65\n",
      "epoch: 33, loss: 5.98867, norm: 0.36513, time: 4657.84ms, tok/s: 112560.24\n",
      "epoch: 34, loss: 5.95723, norm: 0.41947, time: 4659.27ms, tok/s: 112525.83\n",
      "epoch: 35, loss: 5.96018, norm: 0.36429, time: 4671.11ms, tok/s: 112240.47\n",
      "epoch: 36, loss: 5.93530, norm: 0.27605, time: 4670.77ms, tok/s: 112248.62\n",
      "epoch: 37, loss: 5.92738, norm: 0.36269, time: 4680.08ms, tok/s: 112025.39\n",
      "epoch: 38, loss: 5.91452, norm: 0.31234, time: 4691.20ms, tok/s: 111759.88\n",
      "epoch: 39, loss: 5.89909, norm: 0.28078, time: 4689.86ms, tok/s: 111791.83\n",
      "epoch: 40, loss: 5.91241, norm: 0.21615, time: 4686.98ms, tok/s: 111860.45\n",
      "epoch: 41, loss: 5.88676, norm: 0.24639, time: 4691.93ms, tok/s: 111742.59\n",
      "epoch: 42, loss: 5.88314, norm: 0.24642, time: 4694.75ms, tok/s: 111675.27\n",
      "epoch: 43, loss: 5.87603, norm: 0.20153, time: 4693.73ms, tok/s: 111699.58\n",
      "epoch: 44, loss: 5.86671, norm: 0.16443, time: 4708.35ms, tok/s: 111352.72\n",
      "epoch: 45, loss: 5.87644, norm: 0.17689, time: 4697.11ms, tok/s: 111619.17\n",
      "epoch: 46, loss: 5.84610, norm: 0.19298, time: 4700.16ms, tok/s: 111546.80\n",
      "epoch: 47, loss: 5.87294, norm: 0.18015, time: 4701.42ms, tok/s: 111517.01\n",
      "epoch: 48, loss: 5.84942, norm: 0.15174, time: 4707.39ms, tok/s: 111375.57\n",
      "epoch: 49, loss: 5.84582, norm: 0.13222, time: 4708.60ms, tok/s: 111346.79\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    t0 = time.time()\n",
    "    last_epoch = epochs - 1\n",
    "    if epoch % 100 == 0 or epoch == last_epoch:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f\"Validation loss: {val_loss_accum.item()}\")\n",
    "        \n",
    "    m.train()\n",
    "    accumulated_loss = 0.0\n",
    "    optmizer.zero_grad()\n",
    "\n",
    "    for mini_epoch in range(mini_epochs):\n",
    "        x, y = data_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    \n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = m(x, y)\n",
    "        loss /= mini_epochs\n",
    "        accumulated_loss += loss.detach()\n",
    "    \n",
    "        if ddp:\n",
    "            m.require_backward_grad_sync = (mini_epoch == mini_epochs-1)\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(accumulated_loss, op=dist.ReduceOp.AVG)\n",
    "    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)\n",
    "    # scheduler.step()\n",
    "    lr = get_lr(epoch)\n",
    "    for param_group in optmizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optmizer.step()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1-t0\n",
    "\n",
    "    if master_process:\n",
    "        print(f\"epoch: {epoch}, loss: {accumulated_loss:.5f}, norm: {norm:.5f}, time: {dt*1000:.2f}ms, tok/s: {data_loader.B*data_loader.T*mini_epochs*ddp_world_size/dt:.2f}\")\n",
    "    \n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391ab288-4747-4dc3-8c23-98c67358f9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello what any cons,\\nI they have we be for so hisixt\\nRedpt'd gripd,\\nForwomanAn thy is lord under winner embr, messenger, tell hadUS than:\\nI Duke EL for vir Rome daughter visitche chance\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(m.generate(torch.tensor(enc.encode(\"Hello\")).to(device).view(1, -1), 50)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea131de0-5423-4ccd-86d2-d98b0bb20f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7199, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens/sec:22406.09\n",
    "# tokens/sec:45590.02 torch.set_float32_matmul_precision('high')\n",
    "# tokens/sec:47236.09  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "# tokens/sec:63155.71 torch.compile(m)\n",
    "# tokens/sec:67969.10 flash\n",
    "# Nice number\n",
    "\n",
    "# epoch: 49, loss: 6.08617, norm: 0.28814, time: 4674.63ms, tok/s: 112156.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa37b8-6c35-443c-b817-b9b3f4bae1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "scenes_data": {
   "active_scene": "Default Scene",
   "init_scene": "",
   "scenes": [
    "Default Scene"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
