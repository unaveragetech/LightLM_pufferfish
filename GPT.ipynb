{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adc9757b-d90a-4ca3-ac3e-31d308f68b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import tiktoken\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfdfbb9a-8ace-477b-af6b-21be8903dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50304 #50257\n",
    "batch_size = 2**19\n",
    "mini_batches = 8\n",
    "time_stamps = 512\n",
    "context_len = 1024\n",
    "emb_neur = 768\n",
    "epochs = 1\n",
    "num_blocks = 12\n",
    "num_heads = 12\n",
    "# dropout_neur = 0.2\n",
    "data_dir = \"edu_fineweb10B\"\n",
    "log_dir = \"log\"\n",
    "checkpoints_frequency = 2000\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "val_log_file = os.path.join(log_dir, f\"val_log.txt\")\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_lr_steps = 700\n",
    "weight_decay = 0.1\n",
    "beta1, beta2 = 0.9, 0.95\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 \n",
    "if ddp:\n",
    "    init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "else:\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"device: {device}\")\n",
    "    \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ba70c2c-1dba-4868-9fbb-d7fc8e83a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(epoch):\n",
    "    if epoch < warmup_lr_steps:\n",
    "        return (max_lr * (epoch+1)/warmup_lr_steps)\n",
    "    if epoch > epochs:\n",
    "        return min_lr\n",
    "    loc = (epoch - warmup_lr_steps)/(epochs - warmup_lr_steps)\n",
    "    coef = 0.5 * (1.0 + math.cos(math.pi * loc))\n",
    "    return min_lr + coef * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c7aa4be-a0d2-49bc-a705-24b06aeae8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert batch_size % (mini_batches * time_stamps * ddp_world_size) == 0, \"batch_size is not devided by B and T and number_of_gpus\"\n",
    "mini_epochs = int(batch_size / (mini_batches * time_stamps * ddp_world_size)) #number of mini-batches to get 0.5M batch\n",
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, B, T, cur_process, num_processes, data_dir, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.cur_process = cur_process\n",
    "        self.cur_shard = 0\n",
    "        self.num_processes = num_processes\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        shards = os.listdir(self.data_dir)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(self.data_dir, s) for s in shards]\n",
    "        self.shards = shards\n",
    "\n",
    "        self.tokens = load_tokens(self.shards[self.cur_shard])\n",
    "        \n",
    "        self.current_step = cur_process * B * T\n",
    "\n",
    "        print(f\"loaded ~{len(self.tokens)*len(self.shards)} tokens\")\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.cur_process\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        \n",
    "        self.current_step += B * T * self.num_processes\n",
    "        tokens = self.tokens[self.current_step:self.current_step+B*T+1]\n",
    "        x = (tokens[:-1]).view(B, T)\n",
    "        y = (tokens[1:]).view(B, T)\n",
    "        if (self.current_step+B*T* self.num_processes + B*T+1)  > len(self.tokens):\n",
    "            self.cur_shard = (self.cur_shard+1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.cur_shard])\n",
    "            self.current_step = self.cur_process * B * T\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23426d49-2a88-4082-90d8-c93a07006b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(emb_neur, 3 * emb_neur)\n",
    "        self.proj = nn.Linear(emb_neur, emb_neur)\n",
    "        self.proj.COMES_TO_RESIDUAL = 1\n",
    "        # self.dropout = nn.Dropout(dropout_neur)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        assert emb_neur % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        B, T, C = idx.shape\n",
    "        qkv = self.qkv(idx)\n",
    "        q, k, v = qkv.split(emb_neur, dim=2)\n",
    "        q = q.view(B, T, num_heads, C//num_heads).transpose(1, 2) # B, nh, T, hs\n",
    "        k = k.view(B, T, num_heads, C//num_heads).transpose(1, 2) # B, nh, T, hs\n",
    "        v = v.view(B, T, num_heads, C//num_heads).transpose(1, 2) # B, nh, T, hs\n",
    "\n",
    "        # attention = q @ k.transpose(-2, -1) * (1.0 / math.sqrt(k.shape[-1]))\n",
    "        # attention = torch.tril(attention[:, :, :T, :T])\n",
    "        \n",
    "        # attention = attention.masked_fill(attention == 0, float(\"-inf\"))\n",
    "        # attention = F.softmax(attention, dim=-1)\n",
    "        # out = attention @ v # B, nh, T, hs \n",
    "        \n",
    "\n",
    "        attention = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        out = attention.transpose(2, 1).contiguous().view(B, T, C)\n",
    "        out = self.proj(out)\n",
    "        # out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.Linear(emb_neur, 4 * emb_neur),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(4 * emb_neur, emb_neur),\n",
    "        #     nn.Dropout(dropout_neur),\n",
    "        # )\n",
    "        self.upl = nn.Linear(emb_neur, 4 * emb_neur)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dwnl = nn.Linear(4 * emb_neur, emb_neur)\n",
    "        self.dwnl.COMES_TO_RESIDUAL = 1\n",
    "\n",
    "    def forward(self, idx):\n",
    "        idx = self.upl(idx)\n",
    "        idx = self.gelu(idx)\n",
    "        idx = self.dwnl(idx)\n",
    "        return idx\n",
    "        # return self.net(idx)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.attentions = SelfAttention(num_heads)\n",
    "        self.ffn = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(emb_neur)\n",
    "        self.ln2 = nn.LayerNorm(emb_neur)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        idx = idx + self.attentions(self.ln1(idx))\n",
    "        idx = idx + self.ffn(self.ln2(idx))\n",
    "        return idx\n",
    "\n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokens_embedding = nn.Embedding(vocab_size, emb_neur)\n",
    "        self.position_embedding = nn.Embedding(context_len, emb_neur)\n",
    "        self.blocks = nn.Sequential( *[Block(num_heads) for _ in range(num_blocks)])\n",
    "        self.ln = nn.LayerNorm(emb_neur)\n",
    "        self.ll_head = nn.Linear(emb_neur, vocab_size)\n",
    "\n",
    "        self.tokens_embedding.weight = self.ll_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        std = (1.0 / math.sqrt(emb_neur))\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, \"COMES_TO_RESIDUAL\"):\n",
    "                std *= (1.0)/(math.sqrt(2*num_blocks))\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    # I have taken this function [configure_optimizers] from Karpathy's nanoGPT\n",
    "    # https://github.com/karpathy/nanoGPT\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        embedded_tokens = self.tokens_embedding(idx) # B, T, emb_neur\n",
    "        embedded_position = self.position_embedding(torch.arange(T, device=device)) # T, emb_neur\n",
    "        \n",
    "        idx = embedded_tokens + embedded_position # B, T, emb_neur\n",
    "        idx = self.blocks(idx)\n",
    "        idx = self.ln(idx)\n",
    "        logits = self.ll_head(idx)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            logits, _ = self.forward(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f12ce39f-92fb-4cb9-b8f3-ff8b24600422",
   "metadata": {},
   "outputs": [
    {
     "ename": "DeferredCudaCallError",
     "evalue": "CUDA call failed lazily at initialization with error: module 'torch' has no attribute 'version'\n\nCUDA call was originally invoked at:\n\n['  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\runpy.py\", line 196, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\runpy.py\", line 86, in _run_code\\n    exec(code, run_globals)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel_launcher.py\", line 18, in <module>\\n    app.launch_new_instance()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\traitlets\\\\config\\\\application.py\", line 1075, in launch_instance\\n    app.start()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py\", line 739, in start\\n    self.io_loop.start()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\tornado\\\\platform\\\\asyncio.py\", line 195, in start\\n    self.asyncio_loop.run_forever()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\asyncio\\\\base_events.py\", line 603, in run_forever\\n    self._run_once()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\asyncio\\\\base_events.py\", line 1909, in _run_once\\n    handle._run()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\asyncio\\\\events.py\", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelbase.py\", line 545, in dispatch_queue\\n    await self.process_one()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelbase.py\", line 534, in process_one\\n    await dispatch(*args)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelbase.py\", line 437, in dispatch_shell\\n    await result\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\ipkernel.py\", line 362, in execute_request\\n    await super().execute_request(stream, ident, parent)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelbase.py\", line 778, in execute_request\\n    reply_content = await reply_content\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\ipkernel.py\", line 449, in do_execute\\n    res = shell.run_cell(\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\zmqshell.py\", line 549, in run_cell\\n    return super().run_cell(*args, **kwargs)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\interactiveshell.py\", line 3075, in run_cell\\n    result = self._run_cell(\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\interactiveshell.py\", line 3130, in _run_cell\\n    result = runner(coro)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\async_helpers.py\", line 128, in _pseudo_sync_runner\\n    coro.send(None)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\interactiveshell.py\", line 3334, in run_cell_async\\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\interactiveshell.py\", line 3517, in run_ast_nodes\\n    if await self.run_code(code, result, async_=asy):\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\interactiveshell.py\", line 3577, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_24604\\\\2621092005.py\", line 1, in <module>\\n    import torch\\n', '  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\__init__.py\", line 1146, in <module>\\n    _C._initExtension(manager_path())\\n', '  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\cuda\\\\__init__.py\", line 197, in <module>\\n    _lazy_call(_check_capability)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\cuda\\\\__init__.py\", line 195, in _lazy_call\\n    _queued_calls.append((callable, traceback.format_stack()))\\n']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:260\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m     \u001b[43mqueued_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:142\u001b[0m, in \u001b[0;36m_check_capability\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m old_gpu_warn \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124mFound GPU\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m which is of cuda capability \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124mPyTorch no longer supports this GPU because it is too old.\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124mThe minimum cuda capability supported by this library is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# on ROCm we don't want this check\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     CUDA_VERSION \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getCompiledVersion()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'version'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDeferredCudaCallError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_float32_matmul_precision(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m m \u001b[38;5;241m=\u001b[39m GPT()\n\u001b[1;32m----> 4\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# # m = torch.compile(m)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# #making loss average from all gpus\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# if ddp:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#     m = DDP(m, device_ids=[ddp_local_rank]) \u001b[39;00m\n\u001b[0;32m      9\u001b[0m raw_m \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;28;01mif\u001b[39;00m ddp \u001b[38;5;28;01melse\u001b[39;00m m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:264\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    262\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call was originally invoked at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00morig_traceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 264\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DeferredCudaCallError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(_tls, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_initializing\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mDeferredCudaCallError\u001b[0m: CUDA call failed lazily at initialization with error: module 'torch' has no attribute 'version'\n\nCUDA call was originally invoked at:\n\n['  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\runpy.py\", line 196, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\runpy.py\", line 86, in _run_code\\n    exec(code, run_globals)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel_launcher.py\", line 18, in <module>\\n    app.launch_new_instance()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\traitlets\\\\config\\\\application.py\", line 1075, in launch_instance\\n    app.start()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py\", line 739, in start\\n    self.io_loop.start()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\tornado\\\\platform\\\\asyncio.py\", line 195, in start\\n    self.asyncio_loop.run_forever()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\asyncio\\\\base_events.py\", line 603, in run_forever\\n    self._run_once()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\asyncio\\\\base_events.py\", line 1909, in _run_once\\n    handle._run()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\asyncio\\\\events.py\", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelbase.py\", line 545, in dispatch_queue\\n    await self.process_one()\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelbase.py\", line 534, in process_one\\n    await dispatch(*args)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelbase.py\", line 437, in dispatch_shell\\n    await result\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\ipkernel.py\", line 362, in execute_request\\n    await super().execute_request(stream, ident, parent)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelbase.py\", line 778, in execute_request\\n    reply_content = await reply_content\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\ipkernel.py\", line 449, in do_execute\\n    res = shell.run_cell(\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel\\\\zmqshell.py\", line 549, in run_cell\\n    return super().run_cell(*args, **kwargs)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\interactiveshell.py\", line 3075, in run_cell\\n    result = self._run_cell(\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\interactiveshell.py\", line 3130, in _run_cell\\n    result = runner(coro)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\async_helpers.py\", line 128, in _pseudo_sync_runner\\n    coro.send(None)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\interactiveshell.py\", line 3334, in run_cell_async\\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\interactiveshell.py\", line 3517, in run_ast_nodes\\n    if await self.run_code(code, result, async_=asy):\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\IPython\\\\core\\\\interactiveshell.py\", line 3577, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_24604\\\\2621092005.py\", line 1, in <module>\\n    import torch\\n', '  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\__init__.py\", line 1146, in <module>\\n    _C._initExtension(manager_path())\\n', '  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\cuda\\\\__init__.py\", line 197, in <module>\\n    _lazy_call(_check_capability)\\n', '  File \"C:\\\\Users\\\\Bogdan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\cuda\\\\__init__.py\", line 195, in _lazy_call\\n    _queued_calls.append((callable, traceback.format_stack()))\\n']"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "m = GPT()\n",
    "m = m.to(device)\n",
    "# # m = torch.compile(m)\n",
    "# #making loss average from all gpus\n",
    "# if ddp:\n",
    "#     m = DDP(m, device_ids=[ddp_local_rank]) \n",
    "raw_m = m.module if ddp else m\n",
    "\n",
    "# print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# data_loader = DataLoader(mini_batches, time_stamps, cur_process=ddp_rank, num_processes=ddp_world_size, data_dir=data_dir, split=\"train\")\n",
    "# val_loader = DataLoader(mini_batches, time_stamps, cur_process=ddp_rank, num_processes=ddp_world_size, data_dir=data_dir, split=\"val\")\n",
    "# # I have taken this function [configure_optimizers] from Karpathy's nanoGPT\n",
    "# optmizer = raw_m.configure_optimizers(weight_decay, max_lr, (beta1, beta2), device)\n",
    "# # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optmizer, T_max=lr_steps, eta_min=min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f0d7e9-84da-4821-9d0b-14f83fe32497",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    t0 = time.time()\n",
    "    last_epoch = epochs - 1\n",
    "    if epoch % 100 == 0 or epoch == last_epoch:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits, loss = m(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f\"Validation loss: {val_loss_accum.item()}\")\n",
    "            with open(val_log_file, \"a\") as f:\n",
    "                f.write(f\"epoch:{epoch} val_loss:{val_loss_accum.item():.5f}\\n\")\n",
    "            if epoch > 0 and (epoch % checkpoints_frequency == 0 or last_epoch):\n",
    "                checkpoint_path = os.path.join(log_dir, f\"model_{epoch:05d}.pt\")\n",
    "                checkpoint = {\n",
    "                    'model': raw_m.state_dict(),\n",
    "                    'optimizer':optmizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'val_loss': val_loss_accum.item()\n",
    "                }\n",
    "\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "    m.train()\n",
    "    accumulated_loss = 0.0\n",
    "    optmizer.zero_grad()\n",
    "\n",
    "    for mini_epoch in range(mini_epochs):\n",
    "        x, y = data_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    \n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = m(x, y)\n",
    "        loss /= mini_epochs\n",
    "        accumulated_loss += loss.detach()\n",
    "    \n",
    "        if ddp:\n",
    "            m.require_backward_grad_sync = (mini_epoch == mini_epochs-1)\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(accumulated_loss, op=dist.ReduceOp.AVG)\n",
    "    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)\n",
    "    # scheduler.step()\n",
    "    lr = get_lr(epoch)\n",
    "    for param_group in optmizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optmizer.step()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1-t0\n",
    "\n",
    "    if master_process and epoch%5==0:\n",
    "        print(f\"epoch: {epoch}, loss: {accumulated_loss:.5f}, norm: {norm:.5f}, time: {dt*1000:.2f}ms, tok/s: {data_loader.B*data_loader.T*mini_epochs*ddp_world_size/dt:.2f}\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"epoch:{epoch} loss:{accumulated_loss.item():.5f}\\n\")\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ab288-4747-4dc3-8c23-98c67358f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.decode(m.generate(torch.tensor(enc.encode(\"Hello\")).to(device).view(1, -1), 50)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea131de0-5423-4ccd-86d2-d98b0bb20f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens/sec:22406.09\n",
    "# tokens/sec:45590.02 torch.set_float32_matmul_precision('high')\n",
    "# tokens/sec:47236.09  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "# tokens/sec:63155.71 torch.compile(m)\n",
    "# tokens/sec:67969.10 flash\n",
    "# Nice number\n",
    "\n",
    "# epoch: 49, loss: 6.08617, norm: 0.28814, time: 4674.63ms, tok/s: 112156.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ffa37b8-6c35-443c-b817-b9b3f4bae1e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m raww_m \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;28;01mif\u001b[39;00m ddp \u001b[38;5;28;01melse\u001b[39;00m m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load checkpoint\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m## Load checkpoint from file\u001b[39;00m\n\u001b[0;32m     15\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1141\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1139\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m loaded_storages[key]\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m   1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute '_utils'"
     ]
    }
   ],
   "source": [
    "checkpoint_path = './model_19072.pt'  # Adjust path as needed\n",
    "m = GPT()\n",
    "# device = \"cpu\"\n",
    "m = m.to(device)\n",
    "# m = torch.compile(m)\n",
    "#making loss average from all gpus\n",
    "if ddp:\n",
    "    m = DDP(m, device_ids=[ddp_local_rank]) \n",
    "raww_m = m.module if ddp else m\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "## Load checkpoint from file\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Remove \"_orig_mod.\" prefix from the checkpoint keys\n",
    "state_dict = checkpoint['model']\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"_orig_mod.\"):\n",
    "        new_state_dict[k[len(\"_orig_mod.\"):]] = v  # Remove the \"_orig_mod.\" prefix\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "# Load the adjusted state_dict into raww_m\n",
    "raww_m.load_state_dict(new_state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d84e13f-5ef1-417a-ba63-f6cd7ee3f942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nowadays, the Board of Directors, the Union of Municip'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(raww_m.generate(torch.tensor(enc.encode(\"Nowadays\")).to(device).view(1, -1), 10)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbd582-e601-445d-9d8b-57d9e1a30818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "scenes_data": {
   "active_scene": "Default Scene",
   "init_scene": "",
   "scenes": [
    "Default Scene"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
