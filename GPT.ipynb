{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc9757b-d90a-4ca3-ac3e-31d308f68b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import tiktoken\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdfbb9a-8ace-477b-af6b-21be8903dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50304 #50257\n",
    "batch_size = 2**19\n",
    "mini_batches = 8\n",
    "time_stamps = 512\n",
    "context_len = 1024\n",
    "emb_neur = 768\n",
    "epochs = 19073\n",
    "num_blocks = 12\n",
    "num_heads = 12\n",
    "# dropout_neur = 0.2\n",
    "data_dir = \"edu_fineweb10B\"\n",
    "log_dir = \"log\"\n",
    "checkpoints_frequency = 2000\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "val_log_file = os.path.join(log_dir, f\"val_log.txt\")\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_lr_steps = 700\n",
    "weight_decay = 0.1\n",
    "beta1, beta2 = 0.9, 0.95\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 \n",
    "if ddp:\n",
    "    init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "else:\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"device: {device}\")\n",
    "    \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba70c2c-1dba-4868-9fbb-d7fc8e83a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(epoch):\n",
    "    if epoch < warmup_lr_steps:\n",
    "        return (max_lr * (epoch+1)/warmup_lr_steps)\n",
    "    if epoch > epochs:\n",
    "        return min_lr\n",
    "    loc = (epoch - warmup_lr_steps)/(epochs - warmup_lr_steps)\n",
    "    coef = 0.5 * (1.0 + math.cos(math.pi * loc))\n",
    "    return min_lr + coef * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7aa4be-a0d2-49bc-a705-24b06aeae8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert batch_size % (mini_batches * time_stamps * ddp_world_size) == 0, \"batch_size is not devided by B and T and number_of_gpus\"\n",
    "mini_epochs = int(batch_size / (mini_batches * time_stamps * ddp_world_size)) #number of mini-batches to get 0.5M batch\n",
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, B, T, cur_process, num_processes, data_dir, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.cur_process = cur_process\n",
    "        self.cur_shard = 0\n",
    "        self.num_processes = num_processes\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        shards = os.listdir(self.data_dir)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(self.data_dir, s) for s in shards]\n",
    "        self.shards = shards\n",
    "\n",
    "        self.tokens = load_tokens(self.shards[self.cur_shard])\n",
    "        \n",
    "        self.current_step = cur_process * B * T\n",
    "\n",
    "        print(f\"loaded ~{len(self.tokens)*len(self.shards)} tokens\")\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.cur_process\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        \n",
    "        self.current_step += B * T * self.num_processes\n",
    "        tokens = self.tokens[self.current_step:self.current_step+B*T+1]\n",
    "        x = (tokens[:-1]).view(B, T)\n",
    "        y = (tokens[1:]).view(B, T)\n",
    "        if (self.current_step+B*T* self.num_processes + B*T+1)  > len(self.tokens):\n",
    "            self.cur_shard = (self.cur_shard+1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.cur_shard])\n",
    "            self.current_step = self.cur_process * B * T\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23426d49-2a88-4082-90d8-c93a07006b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(emb_neur, 3 * emb_neur)\n",
    "        self.proj = nn.Linear(emb_neur, emb_neur)\n",
    "        self.proj.COMES_TO_RESIDUAL = 1\n",
    "        # self.dropout = nn.Dropout(dropout_neur)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        assert emb_neur % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        B, T, C = idx.shape\n",
    "        qkv = self.qkv(idx)\n",
    "        q, k, v = qkv.split(emb_neur, dim=2)\n",
    "        q = q.view(B, T, num_heads, C//num_heads).transpose(1, 2) # B, nh, T, hs\n",
    "        k = k.view(B, T, num_heads, C//num_heads).transpose(1, 2) # B, nh, T, hs\n",
    "        v = v.view(B, T, num_heads, C//num_heads).transpose(1, 2) # B, nh, T, hs\n",
    "\n",
    "        # attention = q @ k.transpose(-2, -1) * (1.0 / math.sqrt(k.shape[-1]))\n",
    "        # attention = torch.tril(attention[:, :, :T, :T])\n",
    "        \n",
    "        # attention = attention.masked_fill(attention == 0, float(\"-inf\"))\n",
    "        # attention = F.softmax(attention, dim=-1)\n",
    "        # out = attention @ v # B, nh, T, hs \n",
    "        \n",
    "\n",
    "        attention = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        out = attention.transpose(2, 1).contiguous().view(B, T, C)\n",
    "        out = self.proj(out)\n",
    "        # out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.Linear(emb_neur, 4 * emb_neur),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(4 * emb_neur, emb_neur),\n",
    "        #     nn.Dropout(dropout_neur),\n",
    "        # )\n",
    "        self.upl = nn.Linear(emb_neur, 4 * emb_neur)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dwnl = nn.Linear(4 * emb_neur, emb_neur)\n",
    "        self.dwnl.COMES_TO_RESIDUAL = 1\n",
    "\n",
    "    def forward(self, idx):\n",
    "        idx = self.upl(idx)\n",
    "        idx = self.gelu(idx)\n",
    "        idx = self.dwnl(idx)\n",
    "        return idx\n",
    "        # return self.net(idx)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.attentions = SelfAttention(num_heads)\n",
    "        self.ffn = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(emb_neur)\n",
    "        self.ln2 = nn.LayerNorm(emb_neur)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        idx = idx + self.attentions(self.ln1(idx))\n",
    "        idx = idx + self.ffn(self.ln2(idx))\n",
    "        return idx\n",
    "\n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokens_embedding = nn.Embedding(vocab_size, emb_neur)\n",
    "        self.position_embedding = nn.Embedding(context_len, emb_neur)\n",
    "        self.blocks = nn.Sequential( *[Block(num_heads) for _ in range(num_blocks)])\n",
    "        self.ln = nn.LayerNorm(emb_neur)\n",
    "        self.ll_head = nn.Linear(emb_neur, vocab_size)\n",
    "\n",
    "        self.tokens_embedding.weight = self.ll_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        std = (1.0 / math.sqrt(emb_neur))\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, \"COMES_TO_RESIDUAL\"):\n",
    "                std *= (1.0)/(math.sqrt(2*num_blocks))\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    # I have taken this function [configure_optimizers] from Karpathy's nanoGPT\n",
    "    # https://github.com/karpathy/nanoGPT\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        embedded_tokens = self.tokens_embedding(idx) # B, T, emb_neur\n",
    "        embedded_position = self.position_embedding(torch.arange(T, device=device)) # T, emb_neur\n",
    "        \n",
    "        idx = embedded_tokens + embedded_position # B, T, emb_neur\n",
    "        idx = self.blocks(idx)\n",
    "        idx = self.ln(idx)\n",
    "        logits = self.ll_head(idx)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            logits, _ = self.forward(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12ce39f-92fb-4cb9-b8f3-ff8b24600422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124.526208 M parameters\n",
      "loaded ~9900000000 tokens\n",
      "loaded ~100000000 tokens\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 99, with 171,648 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "m = GPT()\n",
    "m = m.to(device)\n",
    "m = torch.compile(m)\n",
    "#making loss average from all gpus\n",
    "if ddp:\n",
    "    m = DDP(m, device_ids=[ddp_local_rank]) \n",
    "raw_m = m.module if ddp else m\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "data_loader = DataLoader(mini_batches, time_stamps, cur_process=ddp_rank, num_processes=ddp_world_size, data_dir=data_dir, split=\"train\")\n",
    "val_loader = DataLoader(mini_batches, time_stamps, cur_process=ddp_rank, num_processes=ddp_world_size, data_dir=data_dir, split=\"val\")\n",
    "\n",
    "# I have taken this function [configure_optimizers] from Karpathy's nanoGPT\n",
    "optmizer = raw_m.configure_optimizers(weight_decay, max_lr, (beta1, beta2), device)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optmizer, T_max=lr_steps, eta_min=min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f0d7e9-84da-4821-9d0b-14f83fe32497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 11.411774635314941\n",
      "epoch: 0, loss: 11.41147, norm: 16.88918, time: 34706.86ms, tok/s: 15106.18\n",
      "epoch: 5, loss: 10.63994, norm: 13.42870, time: 4632.75ms, tok/s: 113169.84\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    t0 = time.time()\n",
    "    last_epoch = epochs - 1\n",
    "    # validation loss check + save model weights every 'checkpoints_frequency' steps\n",
    "    if epoch % 100 == 0 or epoch == last_epoch:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits, loss = m(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f\"Validation loss: {val_loss_accum.item()}\")\n",
    "            with open(val_log_file, \"a\") as f:\n",
    "                f.write(f\"epoch:{epoch} val_loss:{val_loss_accum.item():.5f}\\n\")\n",
    "            if epoch > 0 and (epoch % checkpoints_frequency == 0 or last_epoch):\n",
    "                checkpoint_path = os.path.join(log_dir, f\"model_{epoch:05d}.pt\")\n",
    "                checkpoint = {\n",
    "                    'model': raw_m.state_dict(),\n",
    "                    'optimizer':optmizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'val_loss': val_loss_accum.item()\n",
    "                }\n",
    "\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "    m.train()\n",
    "    accumulated_loss = 0.0\n",
    "    optmizer.zero_grad()\n",
    "    # using accumulated loss\n",
    "    for mini_epoch in range(mini_epochs):\n",
    "        x, y = data_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    \n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = m(x, y)\n",
    "        loss /= mini_epochs\n",
    "        accumulated_loss += loss.detach()\n",
    "    \n",
    "        if ddp:\n",
    "            m.require_backward_grad_sync = (mini_epoch == mini_epochs-1)\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(accumulated_loss, op=dist.ReduceOp.AVG)\n",
    "    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)\n",
    "    # scheduler.step()\n",
    "\n",
    "    # change lr\n",
    "    lr = get_lr(epoch)\n",
    "    for param_group in optmizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optmizer.step()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1-t0\n",
    "\n",
    "    # wrtie to the file losses\n",
    "    if master_process and epoch%5==0:\n",
    "        print(f\"epoch: {epoch}, loss: {accumulated_loss:.5f}, norm: {norm:.5f}, time: {dt*1000:.2f}ms, tok/s: {data_loader.B*data_loader.T*mini_epochs*ddp_world_size/dt:.2f}\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"epoch:{epoch} loss:{accumulated_loss.item():.5f}\\n\")\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea131de0-5423-4ccd-86d2-d98b0bb20f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens/sec:22406.09\n",
    "# tokens/sec:45590.02 torch.set_float32_matmul_precision('high')\n",
    "# tokens/sec:47236.09  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "# tokens/sec:63155.71 torch.compile(m)\n",
    "# tokens/sec:67969.10 flash\n",
    "# Nice number\n",
    "\n",
    "# epoch: 49, loss: 6.08617, norm: 0.28814, time: 4674.63ms, tok/s: 112156.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777487da-3f93-4493-996e-103760aadb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "scenes_data": {
   "active_scene": "Default Scene",
   "init_scene": "",
   "scenes": [
    "Default Scene"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
