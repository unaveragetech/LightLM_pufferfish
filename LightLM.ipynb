{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer, ModelConfig\n",
    "from trainer import Trainer, TrainerConfig, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer_id = \"HuggingFaceTB/SmolLM-360M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainerConfig(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    num_epochs = 1,\n",
    "\n",
    "    use_ddp = True,\n",
    "    use_moe = False,\n",
    "    use_lossfreebalance = False,\n",
    "    clean_cuda_cache = True,\n",
    "    use_compile = True,\n",
    "    use_dtype = \"bfloat16\",\n",
    "\n",
    "    seed = 1,\n",
    "    max_seq_len = 1024,\n",
    "    batch_size = 8,\n",
    "    accumulation_steps = 4,\n",
    "    \n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    learning_rate = 5e-4,\n",
    "    betas = (0.90, 0.95),\n",
    "    update_rate = 1e-5,\n",
    "\n",
    "    val_ratio = 0.005,\n",
    "    steps_for_eval = 20,\n",
    "    eval_interval = 100,\n",
    "\n",
    "    checkpoints_frequency = 1,\n",
    "    path_to_checkpoints = \"./model_testing\",\n",
    "\n",
    "    tokenized_dataset_path = \"fineweb-edu_tok-10BT\",\n",
    "    eval_log_file = \"log/eval.txt\",\n",
    ")\n",
    "\n",
    "config = ModelConfig(\n",
    "        vocab_size = tokenizer.vocab_size,\n",
    "\n",
    "        num_dims = 512,\n",
    "        num_heads = 16,\n",
    "        num_kv_heads = 4,\n",
    "        num_layers = 32,\n",
    "        ffn_hidden_dims = 512 * 4,\n",
    "\n",
    "        rmsnorm_eps = 1e-6,\n",
    "        rope_theta = 1e5,\n",
    "    \n",
    "        context_len = 1536,\n",
    "        \n",
    "        use_cache = False,\n",
    "        use_flash = True,\n",
    "        use_moe = False,\n",
    "\n",
    "        moe_num_experts = 2,\n",
    "        moe_active_experts = 2,\n",
    "        moe_eps = 1e-6,\n",
    "        moe_aux_loss_coef = 0.01,\n",
    "        moe_shared_experts = 1,\n",
    "        use_lossfreebalance = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config)\n",
    "\n",
    "data_loader = DataLoader(train_config)\n",
    "trainer = Trainer(train_config, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train(data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
