{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import tiktoken\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from contextlib import nullcontext\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group\n",
    "from datatrove.utils.dataset import DatatroveFolderDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from typing import Tuple, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "SEED = 1337\n",
    "\n",
    "checkpoints_frequency = 2000\n",
    "\n",
    "\n",
    "data_dir = \"edu_fineweb10B\"\n",
    "log_dir = \"log\"\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "val_log_file = os.path.join(log_dir, f\"val_log.txt\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# assert batch_size % (mini_batches * time_stamps) == 0, \"batch_size is not devided by B and T\"\n",
    "# mini_epochs = int(batch_size / (mini_batches * time_stamps)) #number of mini-batches to get 0.5M batch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    device: str\n",
    "    vocab_size: int\n",
    "\n",
    "    num_dims: int                       # number of dimensions\n",
    "    num_heads: int                      # number of query heads\n",
    "    num_kv_heads: int                   # number of key/value heads\n",
    "    num_layers: int                     # total transformer layers\n",
    "    ffn_hidden_dims: int                # hidden dimension for FFN/FFNwMoE\n",
    "\n",
    "    context_len: int                    # maximum context length\n",
    "    use_cache: bool                     # enable KV-caching\n",
    "    use_flash: bool                     # use Flash Attention\n",
    "    use_moe: bool                       # enable mixture-of-experts\n",
    "\n",
    "    moe_num_experts: int                # total number of experts\n",
    "    moe_routed_experts: int             # number of experts per token (top_k)\n",
    "    moe_eps: float = 1e-6               # epsilon for router stability\n",
    "    moe_aux_loss_coef: float = 0.01     # coefficient for auxiliary loss\n",
    "    moe_shared_experts: int = 0         # number of shared experts (DeepSeekMoE)\n",
    "    use_lossfreebalance: bool = False   # use Auxiliary-loss-free load balancing strategy for mixture-of-experts from DeepSeek https://arxiv.org/pdf/2408.15664\n",
    "\n",
    "    rmsnorm_eps: float = 1e-6\n",
    "    rope_theta: float = 1e5\n",
    "\n",
    "    ffn_dim_multiplier: Optional[int] = None    # optional multiplier to compute ffn_hidden_dims\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function for RoPE\n",
    "def repeat_kv(vct: torch.Tensor, n_times: int):\n",
    "    c_batch_size, c_context_len, num_kv_heads, c_dim = vct.shape\n",
    "    if n_times == 1:\n",
    "        return vct\n",
    "    else:\n",
    "        return (\n",
    "            vct[:, :, :, None, :]\n",
    "            .expand(c_batch_size, c_context_len, num_kv_heads, n_times, c_dim)\n",
    "            .reshape(c_batch_size, c_context_len, num_kv_heads * n_times, c_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0):\n",
    "    assert head_dim % 2 == 0, \"dimensions must be divisible by 2\"\n",
    "\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device)\n",
    "    m = torch.arange(seq_len, device=device)\n",
    "\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex\n",
    "\n",
    "def apply_rotary_pos(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(config.num_dims))\n",
    "        self.eps = config.rmsnorm_eps\n",
    "    \n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.g * self._norm(x.float()).type_as(x)\n",
    "    \n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_cache = config.use_cache\n",
    "        self.use_flash = config.use_flash\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.num_kv_heads = config.num_heads if config.num_kv_heads is None else config.num_kv_heads\n",
    "\n",
    "        self.num_rep = self.num_heads // self.num_kv_heads\n",
    "        self.head_dim = config.num_dims // self.num_heads\n",
    "\n",
    "        self.wq = nn.Linear(config.num_dims, config.num_dims, bias=False)\n",
    "        self.wk = nn.Linear(config.num_dims, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(config.num_dims, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(config.num_dims, config.num_dims, bias=False)\n",
    "\n",
    "    def forward(self, x, freqs_complex, start_pos = 0):\n",
    "        c_batch_size, c_context_len, c_dim = x.shape # c_context_len = 1\n",
    "    \n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        q = q.view(c_batch_size, c_context_len, self.num_heads, self.head_dim)      # B, T, qh, hs\n",
    "        k = k.view(c_batch_size, c_context_len, self.num_kv_heads, self.head_dim)   # B, T, kh, hs\n",
    "        v = v.view(c_batch_size, c_context_len, self.num_kv_heads, self.head_dim)   # B, T, vh, hs\n",
    "\n",
    "        queries = apply_rotary_pos(q, freqs_complex, device=x.device)\n",
    "        keys = apply_rotary_pos(k, freqs_complex, device=x.device)\n",
    "\n",
    "        if self.use_cache:\n",
    "            # Initialize cache if not exist\n",
    "            if self.cache_k is None:\n",
    "                self.cache_k = torch.zeros(\n",
    "                    (c_batch_size, self.config.context_len, self.num_kv_heads, self.head_dim),\n",
    "                    device=x.device\n",
    "                )\n",
    "                self.cache_v = torch.zeros(\n",
    "                    (c_batch_size, self.config.context_len, self.num_kv_heads, self.head_dim),\n",
    "                    device=x.device\n",
    "                )\n",
    "            # Update cache\n",
    "            self.cache_k[:c_batch_size, start_pos:start_pos + c_context_len] = keys\n",
    "            self.cache_v[:c_batch_size, start_pos:start_pos + c_context_len] = v\n",
    "\n",
    "            keys = self.cache_k[:c_batch_size, :start_pos + c_context_len]\n",
    "            v = self.cache_v[:c_batch_size, :start_pos + c_context_len]\n",
    "            \n",
    "\n",
    "        if self.use_flash:\n",
    "            output = F.scaled_dot_product_attention(queries, keys, v, is_causal=True, enable_gqa=True)\n",
    "            \n",
    "        else: # Calculate Grouped Query Attention manually\n",
    "            keys = repeat_kv(keys, self.num_rep)\n",
    "            values = repeat_kv(v, self.num_rep)\n",
    "    \n",
    "            queries = queries.transpose(1, 2)\n",
    "            keys = keys.transpose(1, 2)\n",
    "            values = values.transpose(1, 2)\n",
    "    \n",
    "            attention = torch.matmul(queries, keys.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "    \n",
    "            attention = torch.tril(attention[:, :, :c_context_len, :c_context_len])\n",
    "            attention = attention.masked_fill(attention == 0, float(\"-inf\"))\n",
    "    \n",
    "            attention = F.softmax(attention, dim=-1).type_as(queries)\n",
    "            output = torch.matmul(attention, values)\n",
    "\n",
    "        output = output.transpose(2, 1).contiguous().view(c_batch_size, c_context_len, c_dim)\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Default Feed Forward Layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = config.ffn_hidden_dims\n",
    "\n",
    "        self.w1 = nn.Linear(config.num_dims, self.hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(self.hidden_dim, config.num_dims, bias=False)\n",
    "        self.w3 = nn.Linear(config.num_dims, self.hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x)), None\n",
    "\n",
    "\n",
    "class FFNwMoE(nn.Module): \n",
    "    \"\"\"\n",
    "    Feed Forward with MoE with optional shared experts.\n",
    "    Returns after forward:\n",
    "        output: Combined outputs from experts\n",
    "        aux_loss: Auxiliary loss tensor or routing metadata\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.ffn_hidden_dims\n",
    "\n",
    "        self.moe_routed_experts = config.moe_routed_experts # top_k\n",
    "        self.moe_aux_loss_coef = config.moe_aux_loss_coef\n",
    "        self.moe_eps = config.moe_eps\n",
    "        self.moe_shared_experts = config.moe_shared_experts\n",
    "        self.num_experts = config.moe_num_experts\n",
    "\n",
    "        self.use_lossfreebalance = config.use_lossfreebalance \n",
    "\n",
    "\n",
    "        self.router = nn.Linear(config.num_dims, self.num_experts, bias=False)\n",
    "        self.experts = nn.ModuleList()\n",
    "        for _ in range(self.num_experts):\n",
    "            self.experts.append(\n",
    "                nn.ModuleList([\n",
    "                    nn.Linear(config.num_dims, self.hidden_dim, bias=False),\n",
    "                    nn.Linear(self.hidden_dim, config.num_dims, bias=False),\n",
    "                    nn.Linear(config.num_dims, self.hidden_dim, bias=False)\n",
    "                ]))\n",
    "        \n",
    "        # shared experts (for DeepSeekMoE)\n",
    "        self.shared_experts = nn.ModuleList()\n",
    "        for _ in range(self.moe_shared_experts):\n",
    "            self.shared_experts.append(\n",
    "                nn.ModuleList([\n",
    "                    nn.Linear(config.num_dims, self.hidden_dim, bias=False),\n",
    "                    nn.Linear(self.hidden_dim, config.num_dims, bias=False),\n",
    "                    nn.Linear(config.num_dims, self.hidden_dim, bias=False)\n",
    "                ]))\n",
    "            \n",
    "        # Auxiliary-loss-free load balancing strategy for mixture-of-experts from DeepSeek https://arxiv.org/pdf/2408.15664\n",
    "        if self.use_lossfreebalance:\n",
    "            self.expert_biases = nn.Parameter(torch.zeros(self.num_experts))\n",
    "            \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        c_batch_size, c_context_len, c_dim = x.shape\n",
    "        x_flat = x.view(-1, c_dim)          #c_batch_size * c_context_len, c_dim\n",
    "\n",
    "        router_out = self.router(x_flat)\n",
    "        router_probs = F.softmax(router_out, dim=-1) \n",
    "\n",
    "        _, topk_indices = router_out.topk(self.moe_routed_experts, dim=-1)\n",
    "\n",
    "        aux_loss, topk_probs = self._compute_aux_loss(router_out, router_probs, topk_indices)\n",
    "\n",
    "        output = self._compute_expert_outputs(x_flat, topk_indices, topk_probs, router_probs)\n",
    "\n",
    "        return output.view(c_batch_size, c_context_len, c_dim), aux_loss\n",
    "\n",
    "    def _compute_aux_loss(self, router_out, router_probs, topk_indices):\n",
    "        \"\"\"\n",
    "        Computes the auxiliary loss based on whether loss-free balancing is used or not.\n",
    "        \"\"\"\n",
    "        if not self.use_lossfreebalance:\n",
    "            topk_probs, _ = router_probs.topk(self.moe_routed_experts, dim=-1)\n",
    "            expert_mask = F.one_hot(topk_indices[:, 0], self.num_experts).float()\n",
    "            density = expert_mask.mean(dim=0)\n",
    "            router_prob_mean = router_probs.mean(dim=0)\n",
    "            aux_loss = self.moe_aux_loss_coef * torch.sum(density * router_prob_mean) * self.num_experts\n",
    "\n",
    "        else: # if use_lossfreebalance\n",
    "            router_out = router_out + self.expert_biases\n",
    "            router_probs = torch.sigmoid(router_out) # from https://arxiv.org/pdf/2408.15664 paper\n",
    "            topk_probs = router_probs.gather(-1, topk_indices)\n",
    "            topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            # In the case of Auxiliary-loss-free load balancing we pass router_probs, topk_indices as aux_loss for further calculations \n",
    "            aux_loss = (router_probs, topk_indices)\n",
    "        return aux_loss, topk_probs\n",
    "\n",
    "    def _compute_expert_outputs(self, x_flat, topk_indices, topk_probs, router_probs):\n",
    "        \"\"\"\n",
    "        Compute the output of the experts and shared experts if needed\n",
    "        \"\"\"\n",
    "        output = torch.zeros_like(x_flat)\n",
    "\n",
    "        for i in range(self.moe_routed_experts):\n",
    "            expert_index = topk_indices[:, i]\n",
    "            expert_probs = topk_probs[:, i]\n",
    "\n",
    "            for expert_id in range(self.num_experts):\n",
    "                idx = (expert_id == expert_index).nonzero().squeeze()\n",
    "\n",
    "                if idx.numel() == 0:\n",
    "                    continue\n",
    "                x_for_expert = x_flat[idx]\n",
    "                w1, w2, w3 = self.experts[expert_id]\n",
    "                \n",
    "                expert_output = w2(F.silu(w1(x_for_expert)) * w3(x_for_expert))\n",
    "                output[idx] += expert_output * expert_probs[idx].unsqueeze(-1)\n",
    "\n",
    "        # shared experts(for DeepSeekMoE)\n",
    "        for shared_expert_id in range(self.moe_shared_experts):\n",
    "            w1, w2, w3 = self.shared_experts[shared_expert_id]\n",
    "            expert_output = w2(F.silu(w1(x_flat)) * w3(x_flat))\n",
    "            output = output + expert_output\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = GroupedQueryAttention(config)\n",
    "        if config.use_moe:\n",
    "            self.ffn = FFNwMoE(config)\n",
    "        else:\n",
    "            self.ffn = FeedForward(config)\n",
    "\n",
    "\n",
    "        self.norm_attention = RMSNorm(config)\n",
    "        self.norm_ffn = RMSNorm(config)\n",
    "\n",
    "    def forward(self, x, freqs_complex, start_pos):\n",
    "        x = x + self.attention(\n",
    "            self.norm_attention(x), \n",
    "            freqs_complex, \n",
    "            start_pos\n",
    "            )\n",
    "        \n",
    "        ffn_out, aux_loss = self.ffn(\n",
    "            self.norm_ffn(x)\n",
    "            )\n",
    "        x = x + ffn_out\n",
    "        return x, aux_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module, PyTorchModelHubMixin): # extending PyTorchModelHubMixin for save weights as safetensors\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.num_dims = config.num_dims\n",
    "        self.num_heads = config.num_heads\n",
    "        self.num_layers = config.num_layers\n",
    "        self.context_len = config.context_len\n",
    "        self.use_moe = config.use_moe\n",
    "\n",
    "        self.use_lossfreebalance = config.use_lossfreebalance and self.use_moe\n",
    "\n",
    "        # Calculation of hidden_dim for FFN/FFNwMoE\n",
    "        multiple_of = 4\n",
    "        ffn_dim_multiplier = config.ffn_dim_multiplier\n",
    "        hidden_dim = 4 * config.num_dims\n",
    "        hidden_dim = int(2 * config.num_dims / 3)\n",
    "\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "\n",
    "        config.ffn_hidden_dims = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.tokens_embedding = nn.Embedding(self.vocab_size, self.num_dims)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.blocks.append(Block(config))\n",
    "\n",
    "        self.norm = RMSNorm(config)\n",
    "        self.ll_head = nn.Linear(self.num_dims, self.vocab_size, bias=False)\n",
    "\n",
    "        self.tokens_embedding.weight = self.ll_head.weight\n",
    "\n",
    "        self.freqs_complex = precompute_theta_pos_frequencies(self.num_dims // self.num_heads, self.context_len * 2, device=config.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, targets: Optional[torch.Tensor] = None, start_pos: int = 0):\n",
    "        _, seq_len = x.shape\n",
    "        \n",
    "        x = self.tokens_embedding(x)\n",
    "        freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
    "        \n",
    "        total_aux_loss = 0\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x, aux_loss = block(x, freqs_complex=freqs_complex, start_pos=start_pos)\n",
    "            if self.use_moe and not self.use_lossfreebalance:\n",
    "                total_aux_loss += aux_loss\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.ll_head(x)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            ce_loss = None\n",
    "        else:\n",
    "            c_batch_size, c_context_len, c_dim = logits.shape\n",
    "            logits = logits.view(c_batch_size*c_context_len, c_dim)\n",
    "            targets = targets.view(c_batch_size*c_context_len)\n",
    "            ce_loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "            if self.use_moe and not self.use_lossfreebalance: loss = ce_loss + total_aux_loss    # in this case, ce_loss its loss w/o aux_loss\n",
    "            else: # if we want to use Auxiliary-loss-free load balancing we pass router_probs, topk_indices as ce_loss\n",
    "                # Also, work when moe is not used\n",
    "                loss = ce_loss\n",
    "                ce_loss = aux_loss\n",
    "\n",
    "        return logits, loss, ce_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, x: torch.Tensor, max_tokens: int, temperature: float = 1.0, top_k: int = 50, \n",
    "                 use_cache: bool = False):\n",
    "        \"\"\"\n",
    "        Generate text from x up to max_tokens\n",
    "        \"\"\"\n",
    "        for c_tkn_pos in range(max_tokens):\n",
    "            if use_cache:\n",
    "                if c_tkn_pos == 0:\n",
    "                    logits, _, ce_loss = self.forward(x, start_pos=c_tkn_pos)\n",
    "                else:\n",
    "                    logits, _, ce_loss = self.forward(x[:, -1], start_pos=c_tkn_pos)\n",
    "            else:\n",
    "                logits, _, ce_loss = self.forward(x)\n",
    "\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                tkl, idx = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < tkl[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    vocab_size: int\n",
    "    num_epochs: int\n",
    "\n",
    "    use_ddp: bool\n",
    "    use_moe: bool\n",
    "    use_lossfreebalance: bool\n",
    "    clean_cuda_cache: bool = True  # Helps prevent OOM errors during eval on large models\n",
    "    use_compile: bool = True\n",
    "\n",
    "    seed: int = 1998\n",
    "    max_seq_len: int = 1024\n",
    "    batch_size: int = 1\n",
    "    accumulation_steps: int = 1\n",
    "    \n",
    "    weight_decay: float = 0.1\n",
    "    warmup_ratio: float = 0.01\n",
    "    learning_rate: float = 1e-3\n",
    "    betas: Tuple[float, float] = (0.90, 0.95)\n",
    "\n",
    "    val_ratio: int = 0.005\n",
    "    steps_for_eval: int = 20\n",
    "    eval_interval: int = 50\n",
    "\n",
    "    checkpoints_frequency: int = 500\n",
    "    path_to_checkpoints: str = \"./model_testing\"\n",
    "\n",
    "    tokenized_dataset_path: str = \"wiki_hindi_tok/\"\n",
    "    eval_eval_log_file: str = \"logs/eval.txt\"\n",
    "\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, config, rank=0, world_size=1):\n",
    "        self.config = config\n",
    "        self.current_epoch = 0\n",
    "        self.seed = config.seed\n",
    "        self.token_size = 2 if config.vocab_size < 65535 else 4\n",
    "\n",
    "        self.load_dataset(self.seed)\n",
    "        self.len_dataset = len(self.dataset)\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"{'Total tokens loaded: '} {self.len_dataset * config.max_seq_len:,}\")\n",
    "\n",
    "        self.train_len_dataset = math.ceil((1-config.val_ratio) * self.len_dataset)\n",
    "        self.val_len_dataset = self.len_dataset - self.train_len_dataset\n",
    "\n",
    "        shard_size = self.len_dataset // world_size \n",
    "        self.train_start_idx = rank * shard_size\n",
    "        self.train_end_idx = self.train_start_idx + shard_size\n",
    "        self.train_current_idx = self.train_start_idx\n",
    "\n",
    "        self.val_start_idx = self.train_len_dataset\n",
    "        self.val_current_idx = self.val_start_idx\n",
    "\n",
    "    def get_batch(self, current_idx: int, start_idx: int, end_idx: int):\n",
    "        new_idx = current_idx + self.config.batch_size\n",
    "        \n",
    "        x_l, y_l = zip(*[(self.dataset[idx]['input_ids'][:-1], self.dataset[idx]['input_ids'][1:])\n",
    "                    for idx in range(current_idx, min(new_idx, self.len_dataset))])\n",
    "        x, y = torch.stack(list(x_l)), torch.stack(list(y_l))\n",
    "    \n",
    "        if new_idx >= end_idx:\n",
    "            new_idx = start_idx\n",
    "            self.new_epoch()\n",
    "\n",
    "        return x, y, new_idx\n",
    "\n",
    "    def next_batch(self, split):\n",
    "        if split == \"train\":\n",
    "            x, y, self.train_current_idx = self.get_batch(self.train_current_idx, self.train_start_idx, self.train_end_idx)\n",
    "        else: # validation\n",
    "            x, y, self.val_current_idx = self.get_batch(self.val_current_idx, self.val_start_idx, self.len_dataset)\n",
    "        return x, y\n",
    "    \n",
    "    def reset(self, rank: int = 0, world_size: int = 1):\n",
    "        self.current_epoch = 0\n",
    "        self.seed = self.config.seed\n",
    "        self.load_dataset(self.seed)\n",
    "        self.len_dataset = len(self.dataset)\n",
    "\n",
    "        self.val_len_dataset = self.len_dataset - self.train_len_dataset\n",
    "\n",
    "        shard_size = self.len_dataset // world_size \n",
    "        self.train_start_idx = rank * shard_size\n",
    "        self.train_end_idx = self.train_start_idx + shard_size\n",
    "        self.train_current_idx = self.train_start_idx\n",
    "\n",
    "        self.val_start_idx = self.train_len_dataset\n",
    "        self.val_current_idx = self.val_start_idx\n",
    "\n",
    "    def new_epoch(self):\n",
    "        self.current_epoch += 1\n",
    "        self.load_dataset(self.seed + self.current_epoch)\n",
    "\n",
    "    def load_dataset(self, seed: int):\n",
    "        self.dataset = DatatroveFolderDataset(\n",
    "            folder_path=self.config.tokenized_dataset_path,\n",
    "            filename_pattern=os.path.join(self.config.tokenized_dataset_path, \"**\", \"*.ds\"),\n",
    "            seq_len=self.config.max_seq_len,\n",
    "            token_size=self.token_size,\n",
    "            recursive=True,\n",
    "            shuffle=True,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "    def num_train_steps(self):\n",
    "        return math.ceil((self.train_end_idx-self.train_start_idx) / self.config.batch_size)\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, config, model, tokenizer):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.num_epochs = config.num_epochs\n",
    "\n",
    "        self.use_moe = config.use_moe\n",
    "        self.use_lossfreebalance = config.use_lossfreebalance if self.use_moe else False\n",
    "        self.clean_cuda_cache = config.clean_cuda_cache\n",
    "        self.steps_for_eval = config.steps_for_eval\n",
    "        self.weight_decay = config.weight_decay\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        if self.device == 'cuda':\n",
    "            torch.cuda.manual_seed(config.seed)\n",
    "            n_gpus = torch.cuda.device_count()\n",
    "\n",
    "        use_compile = self.config.use_compile and self.device == \"cuda\" and torch.__version__.startswith(\"2\")\n",
    "        if use_compile:\n",
    "            self.model = torch.compile(self.model)\n",
    "            \n",
    "        # DDP\n",
    "        if n_gpus > 1 and config.use_ddp:\n",
    "            self.ddp = True\n",
    "            init_process_group(backend=\"nccl\")\n",
    "            self.ddp_rank = int(os.environ['RANK'])\n",
    "            self.ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "            self.ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "            self.device = f'cuda:{self.ddp_local_rank}'\n",
    "            torch.cuda.set_device(self.device)\n",
    "            self.master_process = self.ddp_rank == 0\n",
    "            \n",
    "            self.model = DDP(self.model, device_ids=[self.ddp_local_rank])\n",
    "            self.raw_m = self.model.module\n",
    "        else:\n",
    "            self.ddp = False\n",
    "            self.ddp_rank = 0\n",
    "            self.ddp_world_size = 1\n",
    "            self.master_process = True\n",
    "            # self.model.to(self.device)\n",
    "\n",
    "        print(\"Device:\", self.device)\n",
    "        print(f\"Model's trainable params: {sum([p.data.numel() for p in self.model.parameters() if p.requires_grad]) / 1e6:.2f}M\")\n",
    "        print(f\"Tokens per step: {self.config.batch_size * self.config.max_seq_len * self.ddp_world_size * self.config.accumulation_steps}\")\n",
    "        print(f\"use {'torch.compile()'}: {use_compile}\")\n",
    "        print(f\"Use MoE: {'Yes ' if self.use_moe else 'No'}\")\n",
    "        if self.use_moe:\n",
    "            print(f\"Number of experts: {self.model.blocks[0].ffn.num_experts}\")\n",
    "            print(f\"Number of used experts during inference: {self.model.blocks[0].ffn.moe_routed_experts}\")\n",
    "            print(f\"Method of aux_loss: {'loss-free-balance' if config.use_lossfreebalance else 'default'}\")\n",
    "            print(f\"Number of parameters will be used during inference: {((sum([p.data.numel() for p in self.model.parameters() if p.requires_grad]) - sum(p.numel() for p in self.model.blocks[0].ffn.parameters()) * len(self.model.blocks) * (self.model.blocks[0].ffn.moe_routed_experts + self.model.blocks[0].ffn.moe_routed_experts) / self.model.blocks[0].ffn.num_experts))  / 1e6:.2f}M\")\n",
    "    \n",
    "    def step(self, data_loader, accumulation_steps: int,\n",
    "              num_tokens: int, split: str = \"train\"):\n",
    "        \"\"\"\n",
    "        Performs single forward/backward pass with gradient accumulation.\n",
    "            Returns: (total_loss, cross_entropy_loss, number_of_processed_tokens)\n",
    "        \"\"\"\n",
    "        x, y = data_loader.next_batch(split=split)\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        num_tokens += torch.numel(x)\n",
    "        with torch.autocast(device_type=self.device, dtype=torch.bfloat16):\n",
    "            _, loss, ce_loss = self.model(x, y)\n",
    "        loss /= accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "        return loss, ce_loss, num_tokens\n",
    "    \n",
    "\n",
    "    def train(self, data_loader):\n",
    "        num_steps_per_epoch = math.ceil(data_loader.num_train_steps() / self.config.accumulation_steps)\n",
    "\n",
    "        # Configuration of optimizer and schedulers\n",
    "        # Using AdamW with cosine decay and warmup - similar to Llama's training setup\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),  \n",
    "            lr=self.config.learning_rate,\n",
    "            betas=self.config.betas,\n",
    "            weight_decay=self.weight_decay,\n",
    "            fused=(self.device==\"cuda\")\n",
    "        )\n",
    "        \n",
    "        warmup_steps = math.floor(self.config.warmup_ratio * num_steps_per_epoch * self.num_epochs)\n",
    "        warmup_factor = lambda step: 0.05 + 0.95 * (step / max(warmup_steps, 1))\n",
    "        warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=warmup_factor\n",
    "        )\n",
    "\n",
    "        cos_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, \n",
    "            T_max=(num_steps_per_epoch * self.num_epochs) - warmup_steps, \n",
    "            eta_min=0.1 * self.config.learning_rate\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer,\n",
    "            schedulers=[warmup_scheduler, cos_scheduler],\n",
    "            milestones=[warmup_steps])\n",
    "\n",
    "        last_step = num_steps_per_epoch - 1\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for step in range(num_steps_per_epoch):\n",
    "                t0 = time.perf_counter()\n",
    "                accumulated_loss = 0.0\n",
    "                num_tokens = 0\n",
    "\n",
    "                ddp_nosync_ctx = self.model.no_sync() if self.ddp else nullcontext()\n",
    "                with ddp_nosync_ctx:\n",
    "                    for _ in range(self.config.accumulation_steps - 1):\n",
    "                        loss, ce_loss, num_tokens = self.step(data_loader, self.config.accumulation_steps, num_tokens, split=\"train\")\n",
    "                        accumulated_loss += loss\n",
    "\n",
    "                loss, ce_loss, num_tokens = self.step(data_loader, self.config.accumulation_steps, num_tokens, split=\"train\")\n",
    "                accumulated_loss += loss.detach()\n",
    "\n",
    "                # Calculate expert biases using Auxiliary Loss-Free Balance method for MoE (https://arxiv.org/pdf/2408.15664)\n",
    "                if self.use_moe and self.use_lossfreebalance: \n",
    "                    for block in range(len(self.model.blocks)):\n",
    "                        expert_counts = torch.bincount(ce_loss[1].flatten(), minlength=self.model.blocks[block].ffn.moe_routed_experts)  \n",
    "                        avg_count = expert_counts.float().mean()\n",
    "                        for i, count in enumerate(expert_counts):\n",
    "                            error = avg_count - count.float()\n",
    "                            self.model.blocks[block].ffn.expert_biases.data[i] += self.update_rate * torch.sign(error)\n",
    "\n",
    "                norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0) #ToDO\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "                t1 = time.perf_counter()\n",
    "\n",
    "                tokens_per_sec = num_tokens / (t1 - t0)\n",
    "\n",
    "                # Logging \n",
    "                if self.master_process:\n",
    "                    print(f\"Epoch: {epoch} | Step: {step} |  loss: {accumulated_loss:.4f} | norm: {norm:.4f} |  tok/s: {tokens_per_sec}\")\n",
    "                \n",
    "                # Evaluation \n",
    "                if self.master_process and ((step>0 and step % self.config.eval_interval == 0) or step == last_step):\n",
    "                    self.model.eval() \n",
    "                    val_loss = self.eval(data_loader)\n",
    "\n",
    "                    with open(self.config.eval_log_file, \"a\") as f:\n",
    "                        f.write(f\"Step: {step * (epoch+1)}, val_loss: {val_loss:.4f}, norm: {norm:.4f}, time: {t1 - t0:.2f}ms, tok/s: {tokens_per_sec:.1f} \\n\")\n",
    "\n",
    "                    self.model.train()\n",
    "                    if self.clean_cuda_cache:\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                # Save Chekpoints\n",
    "                if self.master_process and ((step % self.config.checkpoints_frequency == 0 and step > 0) or step == last_step):\n",
    "                    self.save_checkpoints(self.config.path_to_checkpoints, name=str((epoch+1) * step))\n",
    "\n",
    "    def eval(self, data_loader):\n",
    "        \"\"\"\n",
    "        Evaluates model on validation split using running average of first [steps_for_eval] batches\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            for _ in range(self.steps_for_eval):\n",
    "                with torch.autocast(device_type=self.device, dtype=torch.bfloat16):\n",
    "                    x, y = data_loader.next_batch(split=\"val\")\n",
    "                    x, y = x.to(self.device), y.to(self.device)\n",
    "                    with torch.autocast(device_type=self.device, dtype=torch.bfloat16):\n",
    "                        _, loss, ce_loss = self.model(x, y)\n",
    "                    loss /= self.steps_for_eval\n",
    "                    val_loss_accum += loss.detach()\n",
    "            return val_loss_accum\n",
    "\n",
    "    def save_checkpoints(self, path: str, name: str):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(path, f\"model.checkpoint.{name}.pt\")\n",
    "        # self.model.save_pretrained(\".checkpoint_path\", config=config)\n",
    "        torch.save(self.model.state_dict(), checkpoint_path)\n",
    "        print(\"Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "train_config = TrainerConfig()\n",
    "\n",
    "\n",
    "config = ModelConfig(\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    vocab_size = 50304,\n",
    "\n",
    "    num_dims = 1024,\n",
    "    num_heads = 16,\n",
    "    num_kv_heads = 4,\n",
    "    num_layers = 16,\n",
    "    ffn_hidden_dims = 1024 * 4,\n",
    "\n",
    "    rmsnorm_eps = 1e-6,\n",
    "    rope_theta = 1e5,\n",
    "\n",
    "    context_len = 1024,\n",
    "    \n",
    "    use_cache = False,\n",
    "    use_flash = False,\n",
    "    use_moe = False,\n",
    "\n",
    "    moe_num_experts = 6,\n",
    "    moe_routed_experts = 1,\n",
    "    moe_eps = 1e-6,\n",
    "    moe_aux_loss_coef = 0.01,\n",
    "    moe_shared_experts = 0,\n",
    "    use_lossfreebalance = False,\n",
    "\n",
    ")\n",
    "\n",
    "model = Transformer(config)\n",
    "model = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "tokenizer_id = \"HuggingFaceTB/SmolLM-360M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data_loader = DataLoader(train_config)\n",
    "trainer = Trainer(train_config, model, tokenizer)\n",
    "trainer.train(data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
