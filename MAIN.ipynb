{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a30c400-d56c-4030-9ad0-10f1e2c61763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import tiktoken\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a957f3-e735-4db7-9a32-95e86ee78313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 \n",
    "if ddp:\n",
    "    init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "else:\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"device: {device}\")\n",
    "    \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "def get_lr(epoch):\n",
    "    if epoch < warmup_lr_steps:\n",
    "        return (max_lr * (epoch+1)/warmup_lr_steps)\n",
    "    if epoch > epochs:\n",
    "        return min_lr\n",
    "    loc = (epoch - warmup_lr_steps)/(epochs - warmup_lr_steps)\n",
    "    coef = 0.5 * (1.0 + math.cos(math.pi * loc))\n",
    "    return min_lr + coef * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a217222b-5bdd-421b-b3a1-95fc14ef894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50304 #50257\n",
    "batch_size = 2**19\n",
    "mini_batches = 8\n",
    "time_stamps = 512\n",
    "context_len = 1024\n",
    "\n",
    "data_dir = \"edu_fineweb10B\"\n",
    "log_dir = \"log\"\n",
    "checkpoints_frequency = 2000\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "val_log_file = os.path.join(log_dir, f\"val_log.txt\")\n",
    "\n",
    "assert batch_size % (mini_batches * time_stamps * ddp_world_size) == 0, \"batch_size is not devided by B and T and number_of_gpus\"\n",
    "mini_epochs = int(batch_size / (mini_batches * time_stamps * ddp_world_size)) #number of mini-batches to get 0.5M batch\n",
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, B, T, cur_process, num_processes, data_dir, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.cur_process = cur_process\n",
    "        self.cur_shard = 0\n",
    "        self.num_processes = num_processes\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        shards = os.listdir(self.data_dir)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(self.data_dir, s) for s in shards]\n",
    "        self.shards = shards\n",
    "\n",
    "        self.tokens = load_tokens(self.shards[self.cur_shard])\n",
    "        \n",
    "        self.current_step = cur_process * B * T\n",
    "\n",
    "        print(f\"loaded ~{len(self.tokens)*len(self.shards)} tokens\")\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.cur_process\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        \n",
    "        self.current_step += B * T * self.num_processes\n",
    "        tokens = self.tokens[self.current_step:self.current_step+B*T+1]\n",
    "        x = (tokens[:-1]).view(B, T)\n",
    "        y = (tokens[1:]).view(B, T)\n",
    "        if (self.current_step+B*T* self.num_processes + B*T+1)  > len(self.tokens):\n",
    "            self.cur_shard = (self.cur_shard+1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.cur_shard])\n",
    "            self.current_step = self.cur_process * B * T\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876c4212-ccf7-4579-bc9f-71476c769e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Help functions\n",
    "def repeat_kv(vct, n_times):\n",
    "    bsz, cl, num_kv_heads, dm = vct.shape\n",
    "    if n_times == 1:\n",
    "        return vct\n",
    "    else:\n",
    "        return (\n",
    "            vct[:, :, :, None, :]\n",
    "            .expand(bsz, cl, num_kv_heads, n_times, dm)\n",
    "            .reshape(bsz, cl, num_kv_heads * n_times, dm)\n",
    "        )\n",
    "\n",
    "\n",
    "# https://github.com/hkproj/pytorch-llama/blob/main/model.py\n",
    "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0):\n",
    "    assert head_dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device) # (Dim / 2)\n",
    "    m = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex\n",
    "\n",
    "def apply_rotary_pos(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps : 1e-6):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.g * self._norm(x.float()).type_as(x)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_dim, num_heads, num_kv_heads, batch_size, context_len, use_cache=False, device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        self.num_q_heads = num_heads\n",
    "        self.num_kv_heads = num_heads if num_kv_heads is None else num_kv_heads\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_rep = self.num_q_heads // self.num_kv_heads\n",
    "        self.head_dim = num_dim // self.num_q_heads\n",
    "\n",
    "        self.wq = nn.Linear(num_dim, num_dim, bias=False)\n",
    "        self.wk = nn.Linear(num_dim, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(num_dim, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(num_dim, num_dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                batch_size,\n",
    "                context_len,\n",
    "                self.num_kv_heads,\n",
    "                self.head_dim\n",
    "            ), device=device\n",
    "        )\n",
    "\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                batch_size,\n",
    "                context_len,\n",
    "                self.num_kv_heads,\n",
    "                self.head_dim\n",
    "            ), device=device\n",
    "        )\n",
    "\n",
    "    def forward(self, x, freqs_complex, start_pos = 0):\n",
    "        c_bsz, c_cl, c_dm = x.shape # c_cl = 1\n",
    "    \n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        q = q.view(c_bsz, c_cl, self.num_q_heads, self.head_dim) # B, T, qh, hs\n",
    "        k = k.view(c_bsz, c_cl, self.num_kv_heads, self.head_dim) # B, T, kh, hs\n",
    "        v = v.view(c_bsz, c_cl, self.num_kv_heads, self.head_dim) # B, T, vh, hs\n",
    "\n",
    "        queries = apply_rotary_pos(q, freqs_complex, device=x.device)\n",
    "        keys = apply_rotary_pos(k, freqs_complex, device=x.device)\n",
    "\n",
    "        if self.use_cache:\n",
    "            self.cache_k[:c_bsz, start_pos:start_pos+c_cl] = keys\n",
    "            self.cache_v[:c_bsz, start_pos:start_pos+c_cl] = v\n",
    "            \n",
    "            keys = self.cache_k[:c_bsz, :start_pos+c_cl]\n",
    "            v = self.cache_v[:c_bsz, :start_pos+c_cl]\n",
    "\n",
    "        keys = repeat_kv(keys, self.num_rep)\n",
    "        values = repeat_kv(v, self.num_rep)\n",
    "\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attention = torch.matmul(queries, keys.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "\n",
    "        attention = torch.tril(attention[:, :, :c_cl, :c_cl])\n",
    "        attention = attention.masked_fill(attention == 0, float(\"-inf\"))\n",
    "\n",
    "        attention = F.softmax(attention, dim=-1).type_as(queries)\n",
    "        output = torch.matmul(attention, values)\n",
    "\n",
    "        output = output.transpose(2, 1).contiguous().view(c_bsz, c_cl, c_dm)\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, num_dims, multiple_of, ffn_dim_multiplier=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_dim = 4 * num_dims\n",
    "        hidden_dim = int(2 * num_dims / 3)\n",
    "\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(num_dims, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, num_dims, bias=False)\n",
    "        self.w3 = nn.Linear(num_dims, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_dims, multiple_of, num_heads, num_kv_heads, batch_size, context_len, use_cache=False, eps = 1e-6, ffn_dim_multiplier=None,device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(num_dim=num_dims, num_heads=num_heads, num_kv_heads=num_kv_heads, batch_size=batch_size, context_len=context_len, use_cache=use_cache, device=device)\n",
    "        self.ffn = FeedForward(num_dims=num_dims, multiple_of=multiple_of, ffn_dim_multiplier=ffn_dim_multiplier)\n",
    "\n",
    "        self.norm_attention = RMSNorm(num_dims, eps=eps)\n",
    "        self.norm_ffn = RMSNorm(num_dims, eps=eps)\n",
    "\n",
    "    def forward(self, x, freqs_complex, start_pos):\n",
    "        x = x + self.attention(\n",
    "            self.norm_attention(x), \n",
    "            freqs_complex, \n",
    "            start_pos\n",
    "            )\n",
    "        \n",
    "        x = x + self.ffn(\n",
    "            self.norm_ffn(x)\n",
    "            )\n",
    "        return x\n",
    "\n",
    "        \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, device, vocab_size, batch_size, context_len, num_layers, num_dims, multiple_of, num_heads, num_kv_heads, use_cache=False, eps = 1e-6, ffn_dim_multiplier=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_dims = num_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.context_len = context_len\n",
    "\n",
    "        self.tokens_embedding = nn.Embedding(self.vocab_size, self.num_dims)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.blocks.append(Block(\n",
    "            num_dims=num_dims, multiple_of=multiple_of, num_heads=num_heads, num_kv_heads=num_kv_heads, batch_size=batch_size, context_len=context_len, use_cache=use_cache, eps = 1e-6, ffn_dim_multiplier=ffn_dim_multiplier, device=device\n",
    "            ))\n",
    "\n",
    "        self.norm = RMSNorm(self.num_dims, eps)\n",
    "        self.ll_head = nn.Linear(self.num_dims, self.vocab_size, bias=False)\n",
    "\n",
    "        self.tokens_embedding.weight = self.ll_head.weight\n",
    "\n",
    "        self.freqs_complex = precompute_theta_pos_frequencies(self.num_dims // self.num_heads, self.context_len * 2, device=device)\n",
    "\n",
    "    # I have taken this function [configure_optimizers] from Karpathy's nanoGPT\n",
    "    # https://github.com/karpathy/nanoGPT\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x, targets=None, start_pos=0):\n",
    "        _, seq_len = x.shape\n",
    "        \n",
    "        x = self.tokens_embedding(x) #\n",
    "        freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, freqs_complex=freqs_complex, start_pos=start_pos)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.ll_head(x)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            c_bsz, c_cl, c_dm = logits.shape\n",
    "            logits = logits.view(c_bsz*c_cl, c_dm)\n",
    "            targets = targets.view(c_bsz*c_cl)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, max_tokens, use_cache=False):\n",
    "        for c_tkn_pos in range(max_tokens):\n",
    "            if use_cache:\n",
    "                if c_tkn_pos == 0:\n",
    "                    logits, _ = self.forward(x, start_pos=c_tkn_pos)\n",
    "                else:\n",
    "                    logits, _ = self.forward(x[:, -1], start_pos=c_tkn_pos)\n",
    "            else:\n",
    "                logits, _ = self.forward(x)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "697446d4-acf4-4e95-9e0e-4dbbc876c41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11598aa7-bf20-4c48-bf41-887d68a1f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.108096 M parameters\n",
      "loaded ~9900000000 tokens\n",
      "loaded ~100000000 tokens\n",
      "num decayed parameter tensors: 113, with 127,074,304 parameters\n",
      "num non-decayed parameter tensors: 33, with 33,792 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "m = Transformer(\n",
    "    device, vocab_size, mini_batches, context_len, num_layers=12, num_dims=1024, multiple_of=4, num_heads=16, num_kv_heads=4, use_cache=False, eps = 1e-6, ffn_dim_multiplier=None\n",
    ")\n",
    "m = m.to(device)\n",
    "m = torch.compile(m)\n",
    "#making loss average from all gpus\n",
    "if ddp:\n",
    "    m = DDP(m, device_ids=[ddp_local_rank]) \n",
    "raw_m = m.module if ddp else m\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "data_loader = DataLoader(mini_batches, time_stamps, cur_process=ddp_rank, num_processes=ddp_world_size, data_dir=data_dir, split=\"train\")\n",
    "val_loader = DataLoader(mini_batches, time_stamps, cur_process=ddp_rank, num_processes=ddp_world_size, data_dir=data_dir, split=\"val\")\n",
    "\n",
    "# I have taken this function [configure_optimizers] from Karpathy's nanoGPT\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_lr_steps = 700\n",
    "weight_decay = 0.1\n",
    "beta1, beta2 = 0.9, 0.95\n",
    "\n",
    "optmizer = raw_m.configure_optimizers(weight_decay, max_lr, (beta1, beta2), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b3c7e78-6b0d-469c-b4e6-6f452fc15eb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 281.11 MiB is free. Process 431514 has 28.06 MiB memory in use. Process 738774 has 8.23 GiB memory in use. Process 2920184 has 11.73 GiB memory in use. Of the allocated memory 11.08 GiB is allocated by PyTorch, and 258.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m---> 15\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m val_loss_steps\n\u001b[1;32m     17\u001b[0m val_loss_accum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    470\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 218\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, targets, start_pos)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing fused AdamW: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_fused\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, start_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    219\u001b[0m     _, seq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    221\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_embedding(x) \u001b[38;5;66;03m#\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1100\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1098\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   1099\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 1100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:321\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n\u001b[1;32m    320\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 321\u001b[0m     all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:124\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    128\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:667\u001b[0m, in \u001b[0;36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    664\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_tokens), \u001b[38;5;241m*\u001b[39margs]\n\u001b[1;32m    665\u001b[0m     old_args\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 667\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:488\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    481\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    482\u001b[0m         runtime_metadata,\n\u001b[1;32m    483\u001b[0m         out,\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    485\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/codecache.py:1478\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1477\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/utils.py:1977\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(new_inputs: List[InputType]):\n\u001b[1;32m   1976\u001b[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m-> 1977\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_jovyan/ht/cht4odqop4fxj65d6y3ynqofdqduhl54tnjmngfou4dmadruux7d.py:3855\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   3853\u001b[0m triton_poi_fused__to_copy_20\u001b[38;5;241m.\u001b[39mrun(arg1_1, buf876, \u001b[38;5;241m51511296\u001b[39m, grid\u001b[38;5;241m=\u001b[39mgrid(\u001b[38;5;241m51511296\u001b[39m), stream\u001b[38;5;241m=\u001b[39mstream0)\n\u001b[1;32m   3854\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arg1_1\n\u001b[0;32m-> 3855\u001b[0m buf877 \u001b[38;5;241m=\u001b[39m \u001b[43mempty_strided_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50304\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50304\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3856\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [logits], Original ATen: [aten.mm]\u001b[39;00m\n\u001b[1;32m   3857\u001b[0m extern_kernels\u001b[38;5;241m.\u001b[39mmm(reinterpret_tensor(buf875, (\u001b[38;5;241m4096\u001b[39m, \u001b[38;5;241m1024\u001b[39m), (\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m), reinterpret_tensor(buf876, (\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m50304\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1024\u001b[39m), \u001b[38;5;241m0\u001b[39m), out\u001b[38;5;241m=\u001b[39mbuf877)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 281.11 MiB is free. Process 431514 has 28.06 MiB memory in use. Process 738774 has 8.23 GiB memory in use. Process 2920184 has 11.73 GiB memory in use. Of the allocated memory 11.08 GiB is allocated by PyTorch, and 258.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "epochs = 19000\n",
    "for epoch in range(epochs):\n",
    "    t0 = time.time()\n",
    "    last_epoch = epochs - 1\n",
    "    # validation loss check + save model weights every 'checkpoints_frequency' steps\n",
    "    if epoch % 300 == 0 or epoch == last_epoch:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits, loss = m(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f\"Validation loss: {val_loss_accum.item()}\")\n",
    "            with open(val_log_file, \"a\") as f:\n",
    "                f.write(f\"epoch:{epoch} val_loss:{val_loss_accum.item():.5f}\\n\")\n",
    "            if epoch > 0 and (epoch % checkpoints_frequency == 0 or last_epoch):\n",
    "                checkpoint_path = os.path.join(log_dir, f\"model_{epoch:05d}.pt\")\n",
    "                checkpoint = {\n",
    "                    'model': raw_m.state_dict(),\n",
    "                    'optimizer':optmizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'val_loss': val_loss_accum.item()\n",
    "                }\n",
    "\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "    m.train()\n",
    "    accumulated_loss = 0.0\n",
    "    optmizer.zero_grad()\n",
    "    # using accumulated loss\n",
    "    for mini_epoch in range(mini_epochs):\n",
    "        x, y = data_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    \n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = m(x, y)\n",
    "        loss /= mini_epochs\n",
    "        accumulated_loss += loss.detach()\n",
    "    \n",
    "        if ddp:\n",
    "            m.require_backward_grad_sync = (mini_epoch == mini_epochs-1)\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(accumulated_loss, op=dist.ReduceOp.AVG)\n",
    "    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)\n",
    "    # scheduler.step()\n",
    "\n",
    "    # change lr\n",
    "    lr = get_lr(epoch)\n",
    "    for param_group in optmizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optmizer.step()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1-t0\n",
    "\n",
    "    # wrtie to the file losses\n",
    "    if master_process and epoch%5==0:\n",
    "        print(f\"epoch: {epoch}, loss: {accumulated_loss:.5f}, norm: {norm:.5f}, time: {dt*1000:.2f}ms, tok/s: {data_loader.B*data_loader.T*mini_epochs*ddp_world_size/dt:.2f}\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"epoch:{epoch} loss:{accumulated_loss.item():.5f}\\n\")\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a05e362e-fa3e-4a89-adbc-6d89ed8681cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/TensorCompare.cu:110: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m      2\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(enc\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mraw_m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI am a large language model,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout cache:\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m11\u001b[39m):\n",
      "Cell \u001b[0;32mIn[4], line 268\u001b[0m, in \u001b[0;36mTransformer.generate\u001b[0;34m(self, x, max_tokens, use_cache)\u001b[0m\n\u001b[1;32m    266\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m    267\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 268\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, next_token), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    t0 = time.time()\n",
    "    print(enc.decode(raw_m.generate(torch.tensor(enc.encode(\"I am a large language model,\")).to(device).view(1, -1), 100)[0].tolist()))\n",
    "    print(\"without cache:\", time.time()-t0)\n",
    "    \n",
    "for i in range(11):\n",
    "    raw_m.blocks[i].attention.use_cache = True\n",
    "    \n",
    "try:\n",
    "    raw_m.blocks[-1].attention.use_cache = True\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "for i in range(20):\n",
    "    t0 = time.time()\n",
    "    print(enc.decode(raw_m.generate(torch.tensor(enc.encode(\"I am a large language model,\")).to(device).view(1, -1), 100)[0].tolist()))\n",
    "    print(\"with cache:\",time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c252dca2-dde5-4fb3-9dd1-15c4e2fd6b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    raw_m.blocks[i].attention.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fad9f71-d343-418c-9838-383d67c3c6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m.blocks[8].attention.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4317942-b991-4fd1-a491-e416f2c9b0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177/1643809108.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a large language model ua Sheikh consentos aba have become the youngest so far vividly in that and is called a non-agulation of a work (NADB Assignment 2). Some test had an entirely new one (a smaller emphasis on misalination of novel\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = './log/model_00800.pt'  # Adjust path as needed\n",
    "model_from_checkpoint = m = Transformer(\n",
    "    device, vocab_size, mini_batches, context_len, num_layers=12, num_dims=1024, multiple_of=4, num_heads=16, num_kv_heads=4, use_cache=False, c_kv = None, eps = 1e-6, ffn_dim_multiplier=None\n",
    ")\n",
    "model_from_checkpoint = model_from_checkpoint.to(device)\n",
    "\n",
    "\n",
    "if ddp:\n",
    "    model_from_checkpoint = DDP(model_from_checkpoint, device_ids=[ddp_local_rank]) \n",
    "model_from_checkpoint = model_from_checkpoint.module if ddp else model_from_checkpoint\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "state_dict = checkpoint['model']\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"_orig_mod.\"):\n",
    "        new_state_dict[k[len(\"_orig_mod.\"):]] = v \n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "model_from_checkpoint.load_state_dict(new_state_dict, strict=False)\n",
    "print(enc.decode(model_from_checkpoint.generate(torch.tensor(enc.encode(\"I am a large language model \")).to(device).view(1, -1), 50)[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "scenes_data": {
   "active_scene": "Default Scene",
   "init_scene": "",
   "scenes": [
    "Default Scene"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
