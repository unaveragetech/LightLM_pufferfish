{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import tiktoken\n",
    "import inspect\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "SEED = 1337\n",
    "\n",
    "checkpoints_frequency = 2000\n",
    "\n",
    "\n",
    "data_dir = \"edu_fineweb10B\"\n",
    "log_dir = \"log\"\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "val_log_file = os.path.join(log_dir, f\"val_log.txt\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# assert batch_size % (mini_batches * time_stamps) == 0, \"batch_size is not devided by B and T\"\n",
    "# mini_epochs = int(batch_size / (mini_batches * time_stamps)) #number of mini-batches to get 0.5M batch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int\n",
    "\n",
    "    num_dims: int\n",
    "    num_heads: int\n",
    "    num_kv_heads: int\n",
    "    num_layers: int\n",
    "    ffn_hidden_dims: int\n",
    "\n",
    "    batch_size: int\n",
    "    mini_batches: int\n",
    "    time_stamps: int\n",
    "    context_len: int\n",
    "    use_cache: bool\n",
    "\n",
    "    num_experts: int\n",
    "    moe_topk: int\n",
    "    moe_eps: float = 1e-6\n",
    "    moe_aux_loss_coef: float= 0.007\n",
    "\n",
    "    rmsnorm_eps: float = 1e-6\n",
    "    rope_theta: float = 1e5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "ddp = int(os.environ.get('RANK', -1)) != -1 \n",
    "if ddp:\n",
    "    init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "else:\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"device: {device}\")\n",
    "    \n",
    "\n",
    "def get_lr(epoch, warmup_lr_steps, max_lr, min_lr, epochs):\n",
    "    if epoch < warmup_lr_steps:\n",
    "        return (max_lr * (epoch+1)/warmup_lr_steps)\n",
    "    if epoch > epochs:\n",
    "        return min_lr\n",
    "    loc = (epoch - warmup_lr_steps)/(epochs - warmup_lr_steps)\n",
    "    coef = 0.5 * (1.0 + math.cos(math.pi * loc))\n",
    "    return min_lr + coef * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, B, T, cur_process, num_processes, data_dir, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.cur_process = cur_process\n",
    "        self.cur_shard = 0\n",
    "        self.num_processes = num_processes\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        shards = os.listdir(self.data_dir)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(self.data_dir, s) for s in shards]\n",
    "        self.shards = shards\n",
    "\n",
    "        self.tokens = load_tokens(self.shards[self.cur_shard])\n",
    "        \n",
    "        self.current_step = cur_process * B * T\n",
    "\n",
    "        print(f\"loaded ~{len(self.tokens)*len(self.shards)} tokens\")\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.cur_process\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        \n",
    "        self.current_step += B * T * self.num_processes\n",
    "        tokens = self.tokens[self.current_step:self.current_step+B*T+1]\n",
    "        x = (tokens[:-1]).view(B, T)\n",
    "        y = (tokens[1:]).view(B, T)\n",
    "        if (self.current_step+B*T* self.num_processes + B*T+1)  > len(self.tokens):\n",
    "            self.cur_shard = (self.cur_shard+1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.cur_shard])\n",
    "            self.current_step = self.cur_process * B * T\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(vct: torch.Tensor, n_times: int):\n",
    "    bsz, cl, num_kv_heads, dm = vct.shape\n",
    "    if n_times == 1:\n",
    "        return vct\n",
    "    else:\n",
    "        return (\n",
    "            vct[:, :, :, None, :]\n",
    "            .expand(bsz, cl, num_kv_heads, n_times, dm)\n",
    "            .reshape(bsz, cl, num_kv_heads * n_times, dm)\n",
    "        )\n",
    "\n",
    "\n",
    "# https://github.com/hkproj/pytorch-llama/blob/main/model.py\n",
    "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0):\n",
    "    assert head_dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device) # (Dim / 2)\n",
    "    m = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex\n",
    "\n",
    "def apply_rotary_pos(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(config.num_dims))\n",
    "        self.eps = config.rmsnorm_eps\n",
    "    \n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.g * self._norm(x.float()).type_as(x)\n",
    "    \n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, config, device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_cache = config.use_cache\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.num_kv_heads = config.num_heads if config.num_kv_heads is None else config.num_kv_heads\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.num_rep = self.num_heads // self.num_kv_heads\n",
    "        self.head_dim = config.num_dims // self.num_heads\n",
    "\n",
    "        self.wq = nn.Linear(config.num_dims, config.num_dims, bias=False)\n",
    "        self.wk = nn.Linear(config.num_dims, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(config.num_dims, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(config.num_dims, config.num_dims, bias=False)\n",
    "\n",
    "        # self.cache_k = torch.zeros(\n",
    "        #     (\n",
    "        #         config.batch_size,\n",
    "        #         config.context_len,\n",
    "        #         self.num_kv_heads,\n",
    "        #         self.head_dim\n",
    "        #     ), device=device\n",
    "        # )\n",
    "\n",
    "        # self.cache_v = torch.zeros(\n",
    "        #     (\n",
    "        #         config.batch_size,\n",
    "        #         config.context_len,\n",
    "        #         self.num_kv_heads,\n",
    "        #         self.head_dim\n",
    "        #     ), device=device\n",
    "        # )\n",
    "\n",
    "    def forward(self, x, freqs_complex, start_pos = 0):\n",
    "        c_batch_size, c_context_len, c_dim = x.shape # c_context_len = 1\n",
    "    \n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        q = q.view(c_batch_size, c_context_len, self.num_heads, self.head_dim) # B, T, qh, hs\n",
    "        k = k.view(c_batch_size, c_context_len, self.num_kv_heads, self.head_dim) # B, T, kh, hs\n",
    "        v = v.view(c_batch_size, c_context_len, self.num_kv_heads, self.head_dim) # B, T, vh, hs\n",
    "\n",
    "        queries = apply_rotary_pos(q, freqs_complex, device=x.device)\n",
    "        keys = apply_rotary_pos(k, freqs_complex, device=x.device)\n",
    "\n",
    "        # if self.use_cache:\n",
    "        #     self.cache_k[:c_batch_size, start_pos:start_pos+c_context_len] = keys\n",
    "        #     self.cache_v[:c_batch_size, start_pos:start_pos+c_context_len] = v\n",
    "            \n",
    "        #     keys = self.cache_k[:c_batch_size, :start_pos+c_context_len]\n",
    "        #     v = self.cache_v[:c_batch_size, :start_pos+c_context_len]\n",
    "\n",
    "        keys = repeat_kv(keys, self.num_rep)\n",
    "        values = repeat_kv(v, self.num_rep)\n",
    "\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attention = torch.matmul(queries, keys.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "\n",
    "        attention = torch.tril(attention[:, :, :c_context_len, :c_context_len])\n",
    "        attention = attention.masked_fill(attention == 0, float(\"-inf\"))\n",
    "\n",
    "        attention = F.softmax(attention, dim=-1).type_as(queries)\n",
    "        output = torch.matmul(attention, values)\n",
    "\n",
    "        output = output.transpose(2, 1).contiguous().view(c_batch_size, c_context_len, c_dim)\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        multiple_of=4\n",
    "        ffn_dim_multiplier=None\n",
    "        hidden_dim = 4 * config.num_dims\n",
    "        hidden_dim = int(2 * config.num_dims / 3)\n",
    "\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(config.num_dims, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, config.num_dims, bias=False)\n",
    "        self.w3 = nn.Linear(config.num_dims, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class FFNwMoE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.moe_aux_loss_coef = config.moe_aux_loss_coef\n",
    "        self.moe_eps = config.moe_eps\n",
    "\n",
    "        multiple_of=4\n",
    "        ffn_dim_multiplier=None\n",
    "        hidden_dim = 4 * config.num_dims\n",
    "        hidden_dim = int(2 * config.num_dims / 3)\n",
    "\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.top_k = config.moe_topk\n",
    "\n",
    "        self.num_experts = config.num_experts\n",
    "        self.router = nn.Linear(config.num_dims, self.num_experts, bias=False)\n",
    "        self.experts = nn.ModuleList()\n",
    "        for _ in range(self.num_experts):\n",
    "            self.experts.append(\n",
    "                nn.ModuleList([\n",
    "                    nn.Linear(config.num_dims, hidden_dim, bias=False),\n",
    "                    nn.Linear(hidden_dim, config.num_dims, bias=False),\n",
    "                    nn.Linear(config.num_dims, hidden_dim, bias=False)\n",
    "                ]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        c_batch_size, c_context_len, c_dim = x.shape\n",
    "        x_flat = x.view(-1, c_dim) #c_batch_size * c_context_len, c_dim\n",
    "\n",
    "\n",
    "        router_out = F.softmax(self.router(x_flat), dim=-1)\n",
    "        topk_probs, topk_indices = router_out.topk(self.top_k, dim=-1)\n",
    "        \n",
    "        output = torch.zeros_like(x_flat)\n",
    "\n",
    "        # expert_mask = torch.zeros_like(router_out).scatter(-1, topk_indices, 1.0)\n",
    "        # density = expert_mask.mean(dim=0)\n",
    "        # router_prob_mean = router_out.mean(dim=0)\n",
    "\n",
    "        # cv_expert = density.std() / (density.mean() + self.moe_eps)\n",
    "        # cv_router = router_prob_mean.std() / (router_prob_mean.mean() + self.moe_eps)\n",
    "        \n",
    "        # aux_loss = (cv_expert**2 + cv_router**2) * self.moe_aux_loss_coef\n",
    "\n",
    "        expert_mask = F.one_hot(topk_indices[:, 0], self.num_experts).float()\n",
    "        density = expert_mask.mean(dim=0)\n",
    "        router_prob_mean = router_out.mean(dim=0)\n",
    "        aux_loss = self.moe_aux_loss_coef * torch.sum(density * router_prob_mean) * self.num_experts\n",
    "        \n",
    "        for i in range(self.top_k):\n",
    "            expert_index = topk_indices[:, i]\n",
    "            expert_probs = topk_probs[:, i]\n",
    "\n",
    "            for expert_id in range(self.num_experts):\n",
    "                idx = (expert_id == expert_index).nonzero().squeeze()\n",
    "\n",
    "                if idx.numel() == 0:\n",
    "                    continue\n",
    "                x_for_expert = x_flat[idx]\n",
    "                w1, w2, w3 = self.experts[expert_id]\n",
    "                \n",
    "                expert_output = w2(F.silu(w1(x_for_expert)) * w3(x_for_expert))\n",
    "                output[idx] += expert_output * expert_probs[idx].unsqueeze(-1)\n",
    "\n",
    "        return output.view(c_batch_size, c_context_len, c_dim), aux_loss\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = GroupedQueryAttention(config, device=device)\n",
    "        self.ffn = FFNwMoE(config)\n",
    "\n",
    "        self.norm_attention = RMSNorm(config)\n",
    "        self.norm_ffn = RMSNorm(config)\n",
    "\n",
    "    def forward(self, x, freqs_complex, start_pos):\n",
    "        x = x + self.attention(\n",
    "            self.norm_attention(x), \n",
    "            freqs_complex, \n",
    "            start_pos\n",
    "            )\n",
    "        \n",
    "        ffn_out, aux_loss = self.ffn(\n",
    "            self.norm_ffn(x)\n",
    "            )\n",
    "        x = x + ffn_out\n",
    "        return x, aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.num_dims = config.num_dims\n",
    "        self.num_heads = config.num_heads\n",
    "        self.num_layers = config.num_layers\n",
    "        self.context_len = config.context_len\n",
    "\n",
    "        self.tokens_embedding = nn.Embedding(self.vocab_size, self.num_dims)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.blocks.append(Block(config))\n",
    "\n",
    "        self.norm = RMSNorm(config)\n",
    "        self.ll_head = nn.Linear(self.num_dims, self.vocab_size, bias=False)\n",
    "\n",
    "        self.tokens_embedding.weight = self.ll_head.weight\n",
    "\n",
    "        self.freqs_complex = precompute_theta_pos_frequencies(self.num_dims // self.num_heads, self.context_len * 2, device=device)\n",
    "\n",
    "    # I have taken this function [configure_optimizers] from Karpathy's nanoGPT\n",
    "    # https://github.com/karpathy/nanoGPT\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x, targets=None, start_pos=0):\n",
    "        _, seq_len = x.shape\n",
    "        \n",
    "        x = self.tokens_embedding(x)\n",
    "        freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
    "        \n",
    "        total_aux_loss = 0\n",
    "\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x, aux_loss = block(x, freqs_complex=freqs_complex, start_pos=start_pos)\n",
    "            total_aux_loss += aux_loss\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.ll_head(x)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            c_batch_size, c_context_len, c_dim = logits.shape\n",
    "            logits = logits.view(c_batch_size*c_context_len, c_dim)\n",
    "            targets = targets.view(c_batch_size*c_context_len)\n",
    "            tt_loss = F.cross_entropy(logits, targets)\n",
    "            loss = tt_loss + total_aux_loss\n",
    "\n",
    "        return logits, loss, tt_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, x, max_tokens, use_cache=False):\n",
    "        for c_tkn_pos in range(max_tokens):\n",
    "            if use_cache:\n",
    "                if c_tkn_pos == 0:\n",
    "                    logits, _ = self.forward(x, start_pos=c_tkn_pos)\n",
    "                else:\n",
    "                    logits, _ = self.forward(x[:, -1], start_pos=c_tkn_pos)\n",
    "            else:\n",
    "                logits, _ = self.forward(x)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.108096 M parameters\n",
      "loaded ~400000000 tokens\n",
      "num decayed parameter tensors: 113, with 127,074,304 parameters\n",
      "num non-decayed parameter tensors: 33, with 33,792 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def main():\n",
    "config = ModelConfig(\n",
    "    vocab_size = 50304,\n",
    "\n",
    "    num_dims = 1024,\n",
    "    num_heads = 16,\n",
    "    num_kv_heads = 4,\n",
    "    num_layers = 16,\n",
    "    ffn_hidden_dims = 4 * 1024,\n",
    "\n",
    "    rmsnorm_eps = 1e-6,\n",
    "    rope_theta = 1e5,\n",
    "\n",
    "    batch_size = 2**19,\n",
    "    mini_batches = 2,\n",
    "    time_stamps = 512,\n",
    "    context_len = 1024,\n",
    "    \n",
    "    use_cache = False,\n",
    "\n",
    "    num_experts = 6,\n",
    "    moe_topk = 2,\n",
    "    moe_eps = 1e-6,\n",
    "    moe_aux_loss_coef = 0.007\n",
    ")\n",
    "\n",
    "m = Transformer(config)\n",
    "model = m.to(device)\n",
    "model = torch.compile(model)\n",
    "# if ddp:\n",
    "#     model = DDP(model, device_ids=[ddp_local_rank]) \n",
    "# raw_m = model.module if ddp else model\n",
    "\n",
    "mini_epochs = int(2**19 / (16 * 512))\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "data_loader = DataLoader(config.mini_batches, config.time_stamps, cur_process=ddp_rank, num_processes=ddp_world_size, data_dir=data_dir, split=\"train\")\n",
    "# val_loader = DataLoader(config.mini_batches, config.time_stamps, cur_process=ddp_rank, num_processes=ddp_world_size, data_dir=data_dir, split=\"val\")\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_lr_steps = 700\n",
    "weight_decay = 0.1\n",
    "beta1, beta2 = 0.9, 0.95\n",
    "\n",
    "optmizer = model.configure_optimizers(weight_decay, max_lr, (beta1, beta2), device)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# x, y = data_loader.next_batch()\n",
    "# x, y = x.to(device), y.to(device)\n",
    "# logits, loss = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "for epoch in range(epochs):\n",
    "    t0 = time.time()\n",
    "    last_epoch = epochs - 1\n",
    "\n",
    "    model.train()\n",
    "    accumulated_loss = 0.0\n",
    "    optmizer.zero_grad()\n",
    "    # using accumulated loss\n",
    "    for mini_epoch in range(mini_epochs):\n",
    "        x, y = data_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    \n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss /= mini_epochs\n",
    "        accumulated_loss += loss.detach()\n",
    "    \n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (mini_epoch == mini_epochs-1)\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(accumulated_loss, op=dist.ReduceOp.AVG)\n",
    "    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # change lr\n",
    "    lr = get_lr(epoch, warmup_lr_steps, max_lr, min_lr, epochs)\n",
    "    for param_group in optmizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optmizer.step()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1-t0\n",
    "\n",
    "    # wrtie to the file losses\n",
    "    if master_process and epoch%5==0:\n",
    "        print(f\"epoch: {epoch}, loss: {accumulated_loss:.5f}, norm: {norm:.5f}, time: {dt*1000:.2f}ms, tok/s: {data_loader.B*data_loader.T*mini_epochs*ddp_world_size/dt:.2f}\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"epoch:{epoch} loss:{accumulated_loss.item():.5f}\\n\")\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
